{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon 농업 환경 변화에 따른 작물 병해 진단 AI 경진대회\n",
    "\n",
    "\n",
    "저는 \"convnext_xlarge_384_in22ft1k+Vanilla LSTM\" 구조의 ConvNext과 LSTM 모델을 다음 구성으로 사용했습니다.\n",
    "\n",
    "* ImageNet-22K에 pretrain된 가중치 로드\n",
    "* (384, 384)의 이미지 크기\n",
    "* Train Augmentation : Resize, ShiftScaleRotate, HorizontalFlip, VerticalFlip, RandomRotate90, CLAHE, Sharpen, RandomBrightnessContrast, RandomResizedCrop, Normalize\n",
    "* Valid Augmentation : Resize, HorizontalFlip, VerticalFlip, Normalize\n",
    "* Test Augmentation : Resize, Normalize\n",
    "* 320 길이의 환경 변수 데이터(온도, 습도, 이슬점의 평균, 최고, 최저)\n",
    "* Embedding Vector의 크기는 LSTM : 2048, Image Model : 1024\n",
    "* gamma=2.0인 Focal Loss\n",
    "* 30 epoch까지 2e-7까지 learning rate가 떨어지는 CosineAnnealingLR 스케줄러를 사용한 Adamw 옵티마이저(lr=1e-4,weigt decay=1e-3)\n",
    "* 30 epoch의 5-fold-CV(validation F1-macro Score가 가장 높은 모델을 선택)\n",
    "* AMP를 이용한 학습\n",
    "* 단일 모델의 5개 fold의 예측 확률을 누적 평균한 예측 결과를 최종 결과로 제출했습니다.\n",
    "\n",
    "\n",
    "\n",
    "## Development Environment\n",
    "Ubuntu 18.04.5 LTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "import argparse\n",
    "\n",
    "from dataset import PlantDACON\n",
    "\n",
    "import timm\n",
    "import torch_optimizer as optim\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "import timm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as albu\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class PlantDACON(Dataset):\n",
    "    def __init__(self,\n",
    "                 image_size,\n",
    "                 files,\n",
    "                 csv_files=None,\n",
    "                 avail_features=None,\n",
    "                 label_description=None,\n",
    "                 aug_ver=0,\n",
    "                 tta_transform=None,\n",
    "                 mode='train'):\n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        self.files = files\n",
    "        self.aug_ver = aug_ver\n",
    "        self.avail_features = avail_features\n",
    "        self.csv_files = csv_files\n",
    "        self.csv_feature_check = [0] * len(self.files)\n",
    "        self.csv_features = [None] * len(self.files)\n",
    "        self.max_len = 320\n",
    "        self.label_description = label_description\n",
    "        self.label_encoder = {key: idx for idx, key in enumerate(self.label_description)}\n",
    "        self.base_transform_list = [\n",
    "            albu.Resize(self.image_size, self.image_size),\n",
    "            albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "                                  rotate_limit=30, interpolation=1, border_mode=0,\n",
    "                                  value=0, p=0.5),\n",
    "            albu.HorizontalFlip(p=0.5),\n",
    "            albu.VerticalFlip(p=0.5)\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.tta_transform = tta_transform\n",
    "        self.transform_normalize = [\n",
    "            albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2()\n",
    "        ]\n",
    "        if self.mode == 'train':\n",
    "            # Only-flip\n",
    "            if self.aug_ver == 1:\n",
    "                self.transform = albu.Compose([albu.Resize(self.image_size, self.image_size),\n",
    "                                               albu.HorizontalFlip(p=0.5),\n",
    "                                               albu.VerticalFlip(p=0.5),\n",
    "                                               albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                                               ToTensorV2(),\n",
    "                                               ])\n",
    "            # Base Augmentation\n",
    "            elif self.aug_ver == 2:\n",
    "                self.transform = albu.Compose(\n",
    "                    self.base_transform_list\n",
    "                    + self.transform_normalize)\n",
    "\n",
    "            elif self.aug_ver == 29:\n",
    "                self.transform = albu.Compose(\n",
    "                    self.base_transform_list +\n",
    "                    [\n",
    "                        albu.RandomRotate90(p=1.0),\n",
    "                        albu.CLAHE(clip_limit=2, p=0.25),\n",
    "                        albu.Sharpen(p=0.25),\n",
    "                        albu.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1),\n",
    "                                                      contrast_limit=(-0.1, 0.1), p=0.25),\n",
    "                        albu.RandomResizedCrop(height=self.image_size, width=self.image_size,\n",
    "                                               scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333),\n",
    "                                               interpolation=1, p=1.0),\n",
    "                    ]\n",
    "                    + self.transform_normalize)\n",
    "            else :\n",
    "                self.transform = albu.Compose([albu.Resize(self.image_size, self.image_size),\n",
    "                                               albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                                               ToTensorV2(),\n",
    "                                               ])\n",
    "        elif self.mode == 'valid':\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Resize(self.image_size, self.image_size),\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                albu.VerticalFlip(p=0.5),\n",
    "                albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        elif self.mode == 'test':\n",
    "            if self.tta_transform:\n",
    "                self.transform = albu.Compose(\n",
    "                    [albu.Resize(self.image_size, self.image_size)]+\n",
    "                    [\n",
    "                        self.tta_transform\n",
    "                    ]\n",
    "                    + self.transform_normalize)\n",
    "            else:\n",
    "                self.transform = albu.Compose([\n",
    "                    albu.Resize(self.image_size, self.image_size),\n",
    "                    albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                    ToTensorV2(),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Resize(self.image_size, self.image_size),\n",
    "                albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "\n",
    "        if avail_features:\n",
    "            self.csv_feature_dict = self.make_csv_feature_dict()\n",
    "#             self.csv_feature_dict = {'내부 온도 1 평균': [3.4, 47.3],\n",
    "#                                      '내부 온도 1 최고': [3.4, 47.6],\n",
    "#                                      '내부 온도 1 최저': [3.3, 47.0],\n",
    "#                                      '내부 습도 1 평균': [23.7, 100.0],\n",
    "#                                      '내부 습도 1 최고': [25.9, 100.0],\n",
    "#                                      '내부 습도 1 최저': [0.0, 100.0],\n",
    "#                                      '내부 이슬점 평균': [0.1, 34.5],\n",
    "#                                      '내부 이슬점 최고': [0.2, 34.7],\n",
    "#                                      '내부 이슬점 최저': [0.0, 34.4]}\n",
    "    def make_csv_feature_dict(self):\n",
    "        # 분석에 사용할 feature 선택\n",
    "        temp_csv = pd.read_csv(self.csv_files[0])[self.avail_features]\n",
    "        max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "        # feature 별 최대값, 최솟값 계산\n",
    "        for csv in tqdm(self.csv_files[1:]):\n",
    "            temp_csv = pd.read_csv(csv)[self.avail_features]\n",
    "            temp_csv = temp_csv.replace('-', np.nan).dropna()\n",
    "            if len(temp_csv) == 0:\n",
    "                continue\n",
    "            temp_csv = temp_csv.astype(float)\n",
    "            temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "            max_arr = np.max([max_arr, temp_max], axis=0)\n",
    "            min_arr = np.min([min_arr, temp_min], axis=0)\n",
    "\n",
    "        # feature 별 최대값, 최솟값 dictionary 생성\n",
    "        csv_feature_dict = {self.avail_features[i]: [min_arr[i], max_arr[i]] for i in range(len(self.avail_features))}\n",
    "        return csv_feature_dict\n",
    "\n",
    "    def get_csv_feature_dict(self):\n",
    "        return self.csv_feature_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split('/')[-1]\n",
    "\n",
    "        json_path = f'{file}/{file_name}.json'\n",
    "        image_path = f'{file}/{file_name}.jpg'\n",
    "        if self.avail_features:\n",
    "            if self.csv_feature_check[i] == 0:\n",
    "                csv_path = f'{file}/{file_name}.csv'\n",
    "                df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "                df = df.replace('-', 0)\n",
    "                # MinMax scaling\n",
    "                for col in df.columns:\n",
    "                    df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "                    df[col] = df[col] / (self.csv_feature_dict[col][1] - self.csv_feature_dict[col][0])\n",
    "                # zero padding\n",
    "                pad = np.zeros((self.max_len, len(df.columns)))\n",
    "                length = min(self.max_len, len(df))\n",
    "                pad[-length:] = df.to_numpy()[-length:]\n",
    "                # transpose to sequential data\n",
    "                csv_feature = pad.T\n",
    "                self.csv_features[i] = csv_feature\n",
    "                self.csv_feature_check[i] = 1\n",
    "            else:\n",
    "                csv_feature = self.csv_features[i]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        img = cv2.imread(image_path)\n",
    "        img = self.transform(image=img)\n",
    "\n",
    "        if self.avail_features:\n",
    "            if self.mode in ['train', 'valid']:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    json_file = json.load(f)\n",
    "\n",
    "                crop = json_file['annotations']['crop']\n",
    "                disease = json_file['annotations']['disease']\n",
    "                risk = json_file['annotations']['risk']\n",
    "                label = f'{crop}_{disease}_{risk}'\n",
    "                return {\n",
    "                    'img': img,\n",
    "                    'csv_feature': torch.tensor(csv_feature, dtype=torch.float32),\n",
    "                    'label': torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                return {\n",
    "                    'img': img,\n",
    "                    'csv_feature': torch.tensor(csv_feature, dtype=torch.float32)\n",
    "                }\n",
    "        else:\n",
    "            if self.mode in ['train', 'valid']:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    json_file = json.load(f)\n",
    "\n",
    "                crop = json_file['annotations']['crop']\n",
    "                disease = json_file['annotations']['disease']\n",
    "                risk = json_file['annotations']['risk']\n",
    "                label = f'{crop}_{disease}_{risk}'\n",
    "\n",
    "                return {\n",
    "                    'img': img,\n",
    "                    'label': torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                return {\n",
    "                    'img': img,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self, model_name, class_n, drop_path_rate, mode='train'):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name.lower()\n",
    "        self.class_n = class_n\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.mode = mode\n",
    "        # 모델\n",
    "        if self.model_name == 'convnext_large_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True)\n",
    "        elif self.model_name == 'convnext_base_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True)\n",
    "        else:\n",
    "            if self.drop_path_rate != 0 :\n",
    "                if self.mode == 'train' :\n",
    "                    self.encoder = timm.create_model(self.model_name, pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "                else:\n",
    "                    self.encoder = timm.create_model(model_name, pretrained=True)\n",
    "            else:\n",
    "                self.encoder = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "        names = []\n",
    "        modules = []\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            names.append(name)\n",
    "            modules.append(module)\n",
    "\n",
    "        self.fc_in_features = self.encoder.num_features\n",
    "        print(f'The layer was modified...')\n",
    "\n",
    "        fc_name = names[-1].split('.')\n",
    "\n",
    "        if len(fc_name)==1:\n",
    "            print(f'{getattr(self.encoder,fc_name[0])} -> Linear(in_features={self.fc_in_features}, out_features={class_n}, bias=True)')\n",
    "            setattr(self.encoder, fc_name[0], nn.Linear(self.fc_in_features, class_n))\n",
    "        elif len(fc_name)==2:\n",
    "            print(f'{getattr(getattr(self.encoder,fc_name[0]),fc_name[1])} -> Linear(in_features={self.fc_in_features}, out_features={class_n}, bias=True)')\n",
    "            setattr(getattr(self.encoder,fc_name[0]), fc_name[1], nn.Linear(self.fc_in_features, class_n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LSTM_Decoder(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim, num_features, cnn_features_len, class_n, rate):\n",
    "        super(LSTM_Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(max_len, embedding_dim)\n",
    "        # self.lstm_fc = nn.Linear(embedding_dim * 2 if bidirectional else embedding_dim, 2048)\n",
    "        self.lstm_fc = nn.Linear(num_features*embedding_dim, 2048)\n",
    "        self.final_layer = nn.Linear(cnn_features_len + 2048, class_n)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, enc_out, dec_inp):\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, _ = self.lstm(dec_inp)\n",
    "        hidden = hidden.view(hidden.size(0), -1)\n",
    "        hidden = self.lstm_fc(hidden)\n",
    "        concat = torch.cat([enc_out, hidden], dim=1) # enc_out + hidden\n",
    "        fc_input = concat\n",
    "        output = self.dropout((self.final_layer(fc_input)))\n",
    "        return output\n",
    "\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        return cosine\n",
    "\n",
    "\n",
    "class ArcfaceImageModel(nn.Module):\n",
    "    def __init__(self, model_name, class_n, drop_path_rate, embedding_dim=1024, mode='train', encode=False):\n",
    "        super().__init__()\n",
    "        self.model_name = '_'.join(model_name.lower().split('_')[1:])\n",
    "        self.class_n = class_n\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mode = mode\n",
    "        self.encode = encode\n",
    "        # 모델\n",
    "        if self.model_name == 'convnext_xlarge_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_xlarge_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_xlarge_384_in22ft1k(pretrained=True)\n",
    "        elif self.model_name == 'convnext_large_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True)\n",
    "        elif self.model_name == 'convnext_base_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True)\n",
    "        else:\n",
    "            if self.drop_path_rate != 0 :\n",
    "                if self.mode == 'train' :\n",
    "                    self.encoder = timm.create_model(self.model_name, pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "                else:\n",
    "                    self.encoder = timm.create_model(self.model_name, pretrained=True)\n",
    "            else:\n",
    "                self.encoder = timm.create_model(self.model_name, pretrained=True)\n",
    "\n",
    "        names = []\n",
    "        modules = []\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            names.append(name)\n",
    "            modules.append(module)\n",
    "\n",
    "        self.fc_in_features = self.encoder.num_features\n",
    "        print(f'The layer was modified...')\n",
    "\n",
    "        fc_name = names[-1].split('.')\n",
    "\n",
    "        if len(fc_name)==1:\n",
    "            print(f'{getattr(self.encoder,fc_name[0])} -> Linear(in_features={self.fc_in_features}, out_features={1000}, bias=True)')\n",
    "            setattr(self.encoder, fc_name[0], nn.Linear(self.fc_in_features, 1000))\n",
    "        elif len(fc_name)==2:\n",
    "            print(f'{getattr(getattr(self.encoder,fc_name[0]),fc_name[1])} -> Linear(in_features={self.fc_in_features}, out_features={1000}, bias=True)')\n",
    "            setattr(getattr(self.encoder,fc_name[0]), fc_name[1], nn.Linear(self.fc_in_features, 1000))\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Linear(1000, self.embedding_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.embedding_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.arc_margin_product = ArcMarginProduct(self.embedding_dim, self.class_n)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.neck(x)\n",
    "        logits = self.arc_margin_product(x)\n",
    "        if self.encode :\n",
    "            return x\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "class ImageModel2LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name,\n",
    "            pretrained_model_path,\n",
    "            max_len,\n",
    "            img_embedding_dim,\n",
    "            env_embedding_dim,\n",
    "            num_features,\n",
    "            class_n,\n",
    "            dropout_rate=0.1,\n",
    "            mode='train'\n",
    "    ):\n",
    "        super(ImageModel2LSTMModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.mode = mode\n",
    "        self.class_n = class_n\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_len = max_len\n",
    "        self.img_embedding_dim = img_embedding_dim\n",
    "        self.env_embedding_dim = env_embedding_dim\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # When using new data (existing data + aihub pepper white powder data), 25 when it's not 28\n",
    "        if self.pretrained_model_path :\n",
    "            self.encoder = ArcfaceImageModel(model_name, 25, drop_path_rate=0, embedding_dim=self.img_embedding_dim,\n",
    "                                             mode='test', encode=True)\n",
    "            self.encoder.load_state_dict(torch.load(self.pretrained_model_path)['model_state_dict'])\n",
    "            self.encoder.requires_grad = False\n",
    "        else:\n",
    "            self.encoder = ArcfaceImageModel(model_name, 25, drop_path_rate=0.2, embedding_dim=self.img_embedding_dim,\n",
    "                                             mode='train', encode=True)\n",
    "\n",
    "        self.rnn = LSTM_Decoder(self.max_len, self.env_embedding_dim, self.num_features, cnn_features_len=self.img_embedding_dim, class_n=self.class_n, rate=self.dropout_rate)\n",
    "\n",
    "    def forward(self, img, seq):\n",
    "        cnn_output = self.encoder(img)\n",
    "        output = self.rnn(cnn_output, seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2, factor=0.1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # weight parameter will act as the alpha parameter to balance class weights\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # num_classes = input.shape[-1]\n",
    "        # target = smooth(target.float(), num_classes, self.factor)\n",
    "        ce_loss = F.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/dongkyuk/DOLG-pytorch/blob/main/model/arcface.py\n",
    "\"\"\"\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, scale_factor=45.0, margin=0.10, criterion=None, weight=None):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            if weight:\n",
    "                self.criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss()\n",
    "        self.margin = margin\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, logits, label):\n",
    "        # input is not l2 normalized\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = phi.type(cosine.type())\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros(cosine.size(), device=logits.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "\n",
    "        logit = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        logit *= self.scale_factor\n",
    "\n",
    "        loss = self.criterion(logit, label)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(cut_w // 2, W - cut_w // 2)\n",
    "    cy = np.random.randint(cut_h // 2, H - cut_h // 2)\n",
    "\n",
    "    bbx1 = cx - cut_w // 2\n",
    "    bby1 = cy - cut_h // 2\n",
    "    bbx2 = cx + cut_w // 2\n",
    "    bby2 = cy + cut_h // 2\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size()[2], x.size()[3], lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_logger(save_dir, comment=None):\n",
    "    c_date, c_time = datetime.now().strftime(\"%Y%m%d/%H%M%S\").split('/')\n",
    "    if comment is not None:\n",
    "        if os.path.exists(os.path.join(save_dir, c_date, comment)):\n",
    "            comment += f'_{c_time}'\n",
    "    else:\n",
    "        comment = c_time\n",
    "    log_dir = os.path.join(save_dir, c_date, comment)\n",
    "    log_txt = os.path.join(log_dir, 'log.txt')\n",
    "\n",
    "    os.makedirs(f'{log_dir}/ckpts')\n",
    "    os.makedirs(f'{log_dir}/submissions')\n",
    "    os.makedirs(f'{log_dir}/comparisons')\n",
    "\n",
    "    global logger\n",
    "    logger = logging.getLogger(c_time)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger = logging.getLogger(c_time)\n",
    "\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    h_file = logging.FileHandler(filename=log_txt, mode='a')\n",
    "    h_file.setFormatter(fmt)\n",
    "    h_file.setLevel(logging.INFO)\n",
    "    logger.addHandler(h_file)\n",
    "    logger.info(f'Log directory ... {log_txt}')\n",
    "    return log_dir\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    acc = (pred == real).sum()/real.shape[0]\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return acc, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, warmup_scheduler, scheduler, scaler, epoch, wandb, args):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    total_train_score = 0\n",
    "    batch_iter = tqdm(enumerate(train_loader), 'Training', total=len(train_loader), ncols=150)\n",
    "\n",
    "    # breakpoint()\n",
    "    lam = None\n",
    "    label_a, label_b = None, None\n",
    "    for batch_idx, batch_item in batch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        label = batch_item['label'].cuda()\n",
    "\n",
    "        if epoch <= args.warm_epoch:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        if args.cutmix:\n",
    "            r = np.random.rand(1)\n",
    "            if r < args.cutmix_prob:\n",
    "                img, label_a, label_b, lam = cutmix(img, label)\n",
    "\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    if args.environment_feature:\n",
    "                        csv_feature = batch_item['csv_feature'].cuda()\n",
    "                        output = model(img, csv_feature)\n",
    "                    else:\n",
    "                        output = model(img)\n",
    "            else:\n",
    "                if args.environment_feature:\n",
    "                    csv_feature = batch_item['csv_feature'].cuda()\n",
    "                    output = model(img, csv_feature)\n",
    "                else:\n",
    "                    output = model(img)\n",
    "            if r < args.cutmix_prob:\n",
    "                train_loss = lam * criterion(output, label_a) + (1 - lam) * criterion(output, label_b)\n",
    "            else:\n",
    "                train_loss = criterion(output, label)\n",
    "        else:\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    if args.environment_feature:\n",
    "                        csv_feature = batch_item['csv_feature'].cuda()\n",
    "                        output = model(img, csv_feature)\n",
    "                    else:\n",
    "                        output = model(img)\n",
    "            else:\n",
    "                if args.environment_feature:\n",
    "                    csv_feature = batch_item['csv_feature'].cuda()\n",
    "                    output = model(img, csv_feature)\n",
    "                else:\n",
    "                    output = model(img)\n",
    "\n",
    "            train_loss = criterion(output, label)\n",
    "\n",
    "        if args.amp:\n",
    "            scaler.scale(train_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.scheduler == 'cycle':\n",
    "            if epoch > args.warm_epoch:\n",
    "                scheduler.step()\n",
    "\n",
    "        train_acc, train_score = accuracy_function(label, output)\n",
    "        total_train_loss += train_loss\n",
    "        total_train_acc += train_acc\n",
    "        total_train_score += train_score\n",
    "        log = f'[EPOCH {epoch}] Train Loss : {train_loss.item():.4f}({total_train_loss / (batch_idx + 1):.4f}), '\n",
    "        log += f'Train Acc : {train_acc.item():.4f}({total_train_acc / (batch_idx + 1):.4f}), '\n",
    "        log += f'Train F1 : {train_score.item():.4f}({total_train_score / (batch_idx + 1):.4f})'\n",
    "        if batch_idx+1 == len(batch_iter):\n",
    "            log = f'[EPOCH {epoch}] Train Loss : {total_train_loss / (batch_idx + 1):.4f}, '\n",
    "            log += f'Train Acc : {total_train_acc / (batch_idx + 1):.4f}, '\n",
    "            log += f'Train F1 : {total_train_score / (batch_idx + 1):.4f}, '\n",
    "            log += f\"LR : {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "\n",
    "\n",
    "        batch_iter.set_description(log)\n",
    "        batch_iter.update()\n",
    "\n",
    "\n",
    "\n",
    "    _lr = optimizer.param_groups[0]['lr']\n",
    "    train_mean_loss = total_train_loss / len(batch_iter)\n",
    "    train_mean_acc = total_train_acc / len(batch_iter)\n",
    "    train_mean_f1 = total_train_score / len(batch_iter)\n",
    "\n",
    "    logger.info(log)\n",
    "    batch_iter.close()\n",
    "\n",
    "    if args.wandb:\n",
    "        wandb.log({'train_mean_loss': train_mean_loss, 'lr': _lr, 'train_mean_acc': train_mean_acc, 'train_mean_f1': train_mean_f1}, step=epoch)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid(model, val_loader, criterion, epoch, wandb, args):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    total_val_score = 0\n",
    "\n",
    "    batch_iter = tqdm(enumerate(val_loader), 'Validating', total=len(val_loader), ncols=150)\n",
    "\n",
    "    for batch_idx, batch_item in batch_iter:\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        label = batch_item['label'].cuda()\n",
    "\n",
    "        if args.environment_feature:\n",
    "            csv_feature = batch_item['csv_feature'].cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(img)\n",
    "\n",
    "        val_loss = criterion(output, label)\n",
    "        val_acc, val_score = accuracy_function(label, output)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val_score += val_score\n",
    "\n",
    "        log = f'[EPOCH {epoch}] Valid Loss : {val_loss.item():.4f}({total_val_loss / (batch_idx + 1):.4f}), '\n",
    "        log += f'Valid Acc : {val_acc.item():.4f}({total_val_acc / (batch_idx + 1):.4f}), '\n",
    "        log += f'Valid F1 : {val_score.item():.4f}({total_val_score / (batch_idx + 1):.4f})'\n",
    "        if batch_idx+1 == len(batch_iter):\n",
    "            log = f'[EPOCH {epoch}] Valid Loss : {total_val_loss / (batch_idx + 1):.4f}, '\n",
    "            log += f'Valid Acc : {total_val_acc / (batch_idx + 1):.4f}, '\n",
    "            log += f'Valid F1 : {total_val_score / (batch_idx + 1):.4f}'\n",
    "        batch_iter.set_description(log)\n",
    "        batch_iter.update()\n",
    "\n",
    "    val_mean_loss = total_val_loss / len(batch_iter)\n",
    "    val_mean_acc = total_val_acc / len(batch_iter)\n",
    "    val_mean_f1 = total_val_score / len(batch_iter)\n",
    "    logger.info(log)\n",
    "    batch_iter.set_description(log)\n",
    "    batch_iter.close()\n",
    "\n",
    "    if args.wandb:\n",
    "        wandb.log({'valid_mean_loss': val_mean_loss,'valid_mean_acc': val_mean_acc, 'valid_mean_f1': val_mean_f1}, step=epoch)\n",
    "\n",
    "    return val_mean_loss, val_mean_acc, val_mean_f1\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader, label_decoder, epoch, fold, wandb, args):\n",
    "    model.eval()\n",
    "    batch_iter = tqdm(enumerate(test_loader), 'Testing', total=len(test_loader),\n",
    "                      leave=False)\n",
    "    preds = []\n",
    "    outputs = None\n",
    "    output_path = f'{args.log_dir}/submissions/output_ep{epoch:03d}_fold{fold:02d}_{args.model}.pt'\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (batch, batch_item) in enumerate(batch_iter):\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        if args.environment_feature:\n",
    "            csv_feature = batch_item['csv_feature'].cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(img)\n",
    "        if i == 0:\n",
    "            outputs = output.cpu()\n",
    "        else:\n",
    "            outputs = torch.cat((outputs, output.cpu()), 0)\n",
    "\n",
    "        output = torch.argmax(output, dim=1).clone().cpu().numpy()\n",
    "        preds.extend(output)\n",
    "\n",
    "    preds = np.array([label_decoder[int(val)] for val in preds])\n",
    "    submission = pd.read_csv(f'{args.data_path}/sample_submission.csv')\n",
    "    submission['label'] = preds\n",
    "    submission.to_csv(f'{args.log_dir}/submissions/submission_ep{epoch:03d}_fold{fold:02d}_{args.model}.csv', index=False)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    torch.save(outputs, output_path)\n",
    "    del outputs\n",
    "    if args.wandb:\n",
    "        wandb.log({'preprocess-infer-save-time.': end - start}, step=epoch)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_label(model, loader, label_description, label_decoder, epoch, fold, args):\n",
    "    model.eval()\n",
    "    batch_iter = tqdm(enumerate(loader), 'Predicting', total=len(loader),\n",
    "                      leave=False)\n",
    "    preds = []\n",
    "    answer = []\n",
    "\n",
    "    for batch, batch_item in batch_iter:\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        if args.environment_feature:\n",
    "            csv_feature = batch_item['csv_feature'].cuda()\n",
    "            output = model(img, csv_feature)\n",
    "        else:\n",
    "            output = model(img)\n",
    "\n",
    "        output = torch.argmax(output, dim=1).clone().cpu().numpy()\n",
    "        preds.extend(output)\n",
    "        answer.extend(batch_item['label'])\n",
    "\n",
    "    answer = np.array([label_description[label_decoder[int(val)]] for val in answer])\n",
    "    preds = np.array([label_description[label_decoder[int(val)]] for val in preds])\n",
    "    new_crosstab = pd.crosstab(answer, preds, rownames=['answer'], colnames=['preds'])\n",
    "    new_crosstab.to_csv(f'{args.log_dir}/comparisons/comparison_ep{epoch:03d}_fold{fold:02d}_{args.model}.csv', index=True)\n",
    "    # print(new_crosstab)\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ensemble_5fold_pt(model_name, output_path_list, label_decoder, args):\n",
    "    predict_list = []\n",
    "\n",
    "    for output_path in output_path_list:\n",
    "        outputs = torch.load(output_path)\n",
    "        preds = torch.softmax(outputs, dim=1).clone().detach().cpu().numpy()\n",
    "        predict_list.append(np.array(preds))\n",
    "\n",
    "    # ensemble = np.array(predict_list[0] + predict_list[1] + predict_list[2]) / len(predict_list)\n",
    "    ensemble = np.array(predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    ensemble = np.argmax(ensemble,axis=1)\n",
    "    ensemble = np.array([label_decoder[val] for val in ensemble])\n",
    "    submission = pd.read_csv(f'{args.data_path}/sample_submission.csv')\n",
    "    submission['label'] = ensemble\n",
    "    csv_name = f'submission_avg_{args.data_split.lower()}_{\"_\".join(model_name.split(\"_\"))}_ep{args.epochs:03d}_'\n",
    "    csv_name +=f'{args.image_size}_{args.optimizer}'\n",
    "    csv_name +=f'_aug_{args.aug_ver:03d}_loss_{args.loss}_amp_{args.amp}_csv_{args.environment_feature}.csv'\n",
    "    submission.to_csv(csv_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-dp', '--data_path', type=str, default='./data',\n",
    "#                         help='Data root path.')\n",
    "#     parser.add_argument('-sp', '--save_path', type=str, default='/mldisk/nfs_shared_/dh/weights/plant-dacon',\n",
    "#                         help='Directory where the generated files will be stored')\n",
    "#     parser.add_argument('-c', '--comment', type=str, default=None)\n",
    "\n",
    "#     parser.add_argument('-e', '--epochs', type=int, default=,\n",
    "#                         help='Number of epochs to train the DML network. Default: 30')\n",
    "#     parser.add_argument('-we', '--warm_epoch', type=int, default=0,\n",
    "#                         help='Number of warmup epochs to train the DML network. Default: 0')\n",
    "\n",
    "#     parser.add_argument('-bs', '--batch_size', type=int, default=28,\n",
    "#                         help='The size of the image you want to preprocess. '\n",
    "#                              'Default: 32')\n",
    "#     parser.add_argument('-is', '--image_size', type=int, default=384,\n",
    "#                         help='Variables to resize the image. '\n",
    "#                              'Default: 384')\n",
    "#     parser.add_argument('-nw', '--num_workers', type=int, default=16,\n",
    "#                         help='Number of workers of dataloader')\n",
    "\n",
    "#     # augmentation configs:\n",
    "#     parser.add_argument('-av', '--aug_ver', type=int, default=29,\n",
    "#                         help='Name of Data Augmentation(Refer to dataset.py). Options: \"0 (No Aug) , 1 ~ 20.')\n",
    "#     parser.add_argument('-cm', '--cutmix', type=bool, default=False,\n",
    "#                         help='Cutmix Auemtnation. Default: True.')\n",
    "#     parser.add_argument('-cmp', '--cutmix_prob', type=float, default=0.25,\n",
    "#                         help='Cutmix Auemtnation Probability. Default: 0.25.')\n",
    "\n",
    "\n",
    "#     # loss configs:\n",
    "#     parser.add_argument('-l', '--loss', type=str, default='focal',\n",
    "#                         help='Name of loss function. Options: \"ce, focal, arcface')\n",
    "\n",
    "#     parser.add_argument('-cw', '--class_weights', type=bool, default=False)\n",
    "\n",
    "#     # optimizer configs:\n",
    "#     parser.add_argument('-ot', '--optimizer', type=str, default='adamw',\n",
    "#                         help='Name of Optimizer. Options: \"adam, radam, adamw, adamp, ranger, lamb')\n",
    "#     parser.add_argument('-sc', '--scheduler', type=str, default='cos_base',\n",
    "#                         help='Name of Optimizer. Options: \"cos_base, cos, cos_warm_restart, cycle')\n",
    "#     # Adam\n",
    "#     parser.add_argument('-lr', '--learning_rate', type=float, default=1e-4,\n",
    "#                         help='Learning rate of the DML network. Default: 10^-4')\n",
    "#     parser.add_argument('-wd', '--weight_decay', type=float, default=1e-3,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "\n",
    "#     # Step\n",
    "#     parser.add_argument('-st', '--step_size', type=int, default=3)\n",
    "#     parser.add_argument('-sg', '--step_gamma', type=float, default=0.8)\n",
    "\n",
    "#     # Cycle\n",
    "#     parser.add_argument('-mal', '--max_lr', type=float, default=1e-3,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "\n",
    "#     # Cosine Annealing\n",
    "#     parser.add_argument('-tm', '--tmax', type=int, default=60,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "#     parser.add_argument('-mil', '--min_lr', type=float, default=1e-6,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "\n",
    "#     # data split configs:\n",
    "#     parser.add_argument('-ds', '--data_split', type=str, default='StratifiedKFold',\n",
    "#                         help='Name of Training Data Sampling Strategy. Options: \"Split_base, Stratified, StratifiedKFold, KFold')\n",
    "#     parser.add_argument('-ns', '--n_splits', type=int, default=5,\n",
    "#                         help='The number of datasets(Train,val) to be divided.')\n",
    "#     parser.add_argument('-rs', '--random_seed', type=int, default=42,\n",
    "#                         help='Random Seed')\n",
    "#     parser.add_argument('-vr', '--val_ratio', type=float, default=0.2,\n",
    "#                         help='validation dataset ratio')\n",
    "\n",
    "\n",
    "#     # image model specific configs:\n",
    "#     parser.add_argument('-m', '--model', type=str, default='arc_convnext_xlarge_384_in22ft1k',\n",
    "#                         help='Name of model. Options: Refer to image_model_list.txt. '\n",
    "#                              'Option : \"convnext_large_384_in22ft1k, swin_large_patch4_window12_384_in22k'\n",
    "#                              'arc_convnext_large_384_in22ft1k, arc_swin_large_patch4_window12_384_in22k'\n",
    "#                              'If you want to use arcface loss while learning only the image model, '\n",
    "#                              'set the \"arc_xxx\" model and the \"arcface\" loss function and environment feature to \"FALSE\".'\n",
    "#                              'If you want to use focal loss while learning the multi-modal model, ' \n",
    "#                              'set the \"arc_xxx\" model and the \"focal\" loss function and environment feature to \"True\".')\n",
    "\n",
    "#     parser.add_argument('-dpr', '--drop_path_rate', type=float, default=0.2,\n",
    "#                         help='dropout rate')\n",
    "\n",
    "#     # multi-modal model specific configs:\n",
    "#     parser.add_argument('-ied', '--img_embedding_dim', type=int, default=1024,\n",
    "#                         help='Dimension of the output embedding of image model')\n",
    "#     parser.add_argument('-eed', '--env_embedding_dim', type=int, default=1024,\n",
    "#                         help='Dimension of the of environment feature model')\n",
    "\n",
    "#     parser.add_argument('-ml', '--max_len', type=int, default=320,\n",
    "#                         help='Number of layers')\n",
    "#     parser.add_argument('-dr', '--dropout_rate', type=float, default=0.2,\n",
    "#                         help='dropout rate')\n",
    "\n",
    "#     # feature configs:\n",
    "#     parser.add_argument('-ef', '--environment_feature', type=bool, default=True,\n",
    "#                         help='Whether to use environmental features'\n",
    "#                         )\n",
    "\n",
    "#     # eval configs:\n",
    "#     parser.add_argument('-d', '--dataset', type=str, default='PlantDACON',\n",
    "#                         help='Name of evaluation dataset. Options: \"PlantDACON, PlantVillage')\n",
    "\n",
    "#     # wandb config:\n",
    "#     parser.add_argument('--wandb', type=bool, default=False,\n",
    "#                         help='wandb'\n",
    "#                         )\n",
    "\n",
    "#     # amp config:\n",
    "#     parser.add_argument('--amp', type=bool, default=True,\n",
    "#                         help='amp mode'\n",
    "#                         )\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    import easydict\n",
    " \n",
    "    args = easydict.EasyDict({\n",
    "\n",
    "            \"data_path\": './data',\n",
    "\n",
    "            \"save_path\": './weights/plant-lg-dacon',\n",
    "\n",
    "            \"comment\": None,\n",
    "            \n",
    "            \"epochs\" : 30,\n",
    "            \n",
    "            \"warm_epoch\": 0,\n",
    "\n",
    "            \"batch_size\": 28,\n",
    "\n",
    "            \"image_size\": 384,\n",
    "            \n",
    "            \"num_workers\": 16,\n",
    "            \n",
    "            # augmentation configs:\n",
    "            \"aug_ver\": 29,\n",
    "\n",
    "            \"cutmix\": False,\n",
    "\n",
    "            \"cutmix_prob\": 0.25,\n",
    "            \n",
    "            # loss configs:\n",
    "            \"loss\": 'focal',\n",
    "\n",
    "            \"class_weights\": False,\n",
    "            \n",
    "            # optimizer configs:\n",
    "            \"optimizer\": 'adamw',\n",
    "\n",
    "            \"scheduler\": 'cos_base',\n",
    "\n",
    "            # Adam \n",
    "            \"learning_rate\": 1e-4,\n",
    "\n",
    "            \"weight_decay\": 1e-3,\n",
    "            \n",
    "            # Step\n",
    "            \"step_size\": 3,\n",
    "            \n",
    "            \"step_gamma\": 0.8,\n",
    "            \n",
    "            # Cycle\n",
    "            \"max_lr\": 1e-3,\n",
    "            \n",
    "            # Cosine Annealing\n",
    "            \"tmax\": 60,\n",
    "\n",
    "            \"min_lr\": 1e-6,\n",
    "            \n",
    "            # data split configs:\n",
    "            \"data_split\": 'StratifiedKFold',\n",
    "\n",
    "            \"n_splits\": 5,\n",
    "            \n",
    "            \"random_seed\": 42,\n",
    "            \n",
    "            \"val_ratio\" : 0.2,\n",
    "            \n",
    "            # image model specific configs:\n",
    "            \"model\": \"arc_convnext_xlarge_384_in22ft1k\",\n",
    "\n",
    "            \"drop_path_rate\": 0.2,\n",
    "            \n",
    "            # multi-modal model specific configs:\n",
    "            \"img_embedding_dim\": 1024,\n",
    "            \n",
    "            \"env_embedding_dim\": 1024,\n",
    "            \n",
    "            \"max_len\" : 320,\n",
    "            \n",
    "            \"dropout_rate\" :0.2,\n",
    "            \n",
    "            # feature configs:\n",
    "            \"environment_feature\": True,\n",
    "            \n",
    "            # eval configs:\n",
    "            \"dataset\": 'PlantDACON',\n",
    "            \n",
    "            # wandb config:\n",
    "            \"wandb\": False,\n",
    "            \n",
    "            # amp config:\n",
    "            \"amp\": True,\n",
    "\n",
    "            \n",
    "\n",
    "    })\n",
    "\n",
    "\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    # Data Path\n",
    "    train_data = sorted(glob(f'{args.data_path}/train/*'))\n",
    "    csv_files = sorted(glob(f'{args.data_path}/train/*/*.csv'))\n",
    "    test_data = sorted(glob(f'{args.data_path}/test/*'))\n",
    "    labelsss = pd.read_csv(f'{args.data_path}/train.csv')['label']\n",
    "\n",
    "    folds = []\n",
    "    # Data Split\n",
    "    if args.data_split.lower() == 'split_base':\n",
    "        train_data, val_data = train_test_split(train_data, random_state=args.random_seed, test_size=args.val_ratio, shuffle=True)\n",
    "        folds.append((train_data, val_data))\n",
    "        args.n_split = 1\n",
    "    elif args.data_split.lower() == 'stratified':\n",
    "        train_data, val_data = train_test_split(train_data, random_state=args.random_seed, test_size=args.val_ratio, stratify=labelsss, shuffle=True)\n",
    "        folds.append((train_data, val_data))\n",
    "        args.n_split = 1\n",
    "    elif args.data_split.lower() == 'stratifiedkfold':\n",
    "        train_data = np.array(train_data)\n",
    "        skf = StratifiedKFold(n_splits=args.n_splits, random_state=args.random_seed, shuffle=True)\n",
    "        for train_idx, valid_idx in skf.split(train_data,labelsss.tolist()):\n",
    "            folds.append((train_data[train_idx].tolist(), train_data[valid_idx].tolist()))\n",
    "\n",
    "    elif args.data_split == 'kfold':\n",
    "        train_data = np.array(train_data)\n",
    "        kf = KFold(n_splits=args.n_splits, random_state=args.random_seed, shuffle=True)\n",
    "        for train_idx, valid_idx in kf.split(train_data,labelsss.tolist()):\n",
    "            folds.append((train_data[train_idx].tolist(), train_data[valid_idx].tolist()))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Available CSV feature for Training and Testing\n",
    "    if args.environment_feature:\n",
    "        avail_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고',\n",
    "                        '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "    else:\n",
    "        avail_features = None\n",
    "\n",
    "    # Label Description : Refer to CSV File\n",
    "    label_description = {\n",
    "        \"1_00_0\": \"딸기\",\n",
    "        \"2_00_0\": \"토마토\",\n",
    "        \"2_a5_2\": \"토마토_흰가루병_중기\",\n",
    "        \"3_00_0\": \"파프리카\",\n",
    "        \"3_a9_1\": \"파프리카_흰가루병_초기\",\n",
    "        \"3_a9_2\": \"파프리카_흰가루병_중기\",\n",
    "        \"3_a9_3\": \"파프리카_흰가루병_말기\",\n",
    "        \"3_b3_1\": \"파프리카_칼슘결핍_초기\",\n",
    "        \"3_b6_1\": \"파프리카_다량원소결필(N)_초기\",\n",
    "        \"3_b7_1\": \"파프리카_다량원소결필(P)_초기\",\n",
    "        \"3_b8_1\": \"파프리카_다량원소결필(K)_초기\",\n",
    "        \"4_00_0\": \"오이\",\n",
    "        \"5_00_0\": \"고추\",\n",
    "        \"5_a7_2\": \"고추_탄저병_중기\",\n",
    "        \"5_b6_1\": \"고추_다량원소결필(N)_초기\",\n",
    "        \"5_b7_1\": \"고추_다량원소결필(P)_초기\",\n",
    "        \"5_b8_1\": \"고추_다량원소결필(K)_초기\",\n",
    "        \"6_00_0\": \"시설포도\",\n",
    "        \"6_a11_1\": \"시설포도_탄저병_초기\",\n",
    "        \"6_a11_2\": \"시설포도_탄저병_중기\",\n",
    "        \"6_a12_1\": \"시설포도_노균병_초기\",\n",
    "        \"6_a12_2\": \"시설포도_노균병_중기\",\n",
    "        \"6_b4_1\": \"시설포도_일소피해_초기\",\n",
    "        \"6_b4_3\": \"시설포도_일소피해_말기\",\n",
    "        \"6_b5_1\": \"시설포도_축과병_초기\"\n",
    "    }\n",
    "\n",
    "\n",
    "    label_encoder = {key: idx for idx, key in enumerate(label_description)}\n",
    "    label_decoder = {val: key for key, val in label_encoder.items()}\n",
    "\n",
    "    if args.class_weights :\n",
    "        val_counts = labelsss.value_counts().sort_index().values\n",
    "        class_weights = 1/np.log1p(val_counts)\n",
    "        class_weights = (class_weights / class_weights.sum()) * len(label_encoder.keys())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "\n",
    "    # Criterion\n",
    "    if args.loss == 'ce':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif args.loss == 'focal':\n",
    "        criterion = FocalLoss()\n",
    "    elif args.loss == 'arcface':\n",
    "        criterion = ArcFaceLoss(criterion=FocalLoss(), weight=class_weights)\n",
    "        # criterion = ArcFaceLoss(criterion=FocalLoss())\n",
    "\n",
    "    print(f'The number of datasets separated : {len(folds)}')\n",
    "    best_model_paths = []\n",
    "    output_path_list = []\n",
    "    best_image_model_paths = [None]*len(folds)\n",
    "    \n",
    "    for fold in range(len(folds)):\n",
    "        train_data, val_data = folds[fold]\n",
    "\n",
    "        # Multi-modal Model\n",
    "        if args.environment_feature:\n",
    "            model = ImageModel2LSTMModel(model_name=args.model,\n",
    "                                         pretrained_model_path=best_image_model_paths[fold],\n",
    "                                         max_len=args.max_len,\n",
    "                                         img_embedding_dim=args.img_embedding_dim,\n",
    "                                         env_embedding_dim=args.env_embedding_dim,\n",
    "                                         num_features=len(avail_features),\n",
    "                                         class_n=len(label_encoder.keys()),\n",
    "                                         dropout_rate=0.1,\n",
    "                                         mode='train')\n",
    "\n",
    "        # Only-image Model\n",
    "        else:\n",
    "            if 'arc' in args.model:\n",
    "                model = ArcfaceImageModel(model_name=args.model,\n",
    "                                          class_n=len(label_encoder.keys()),\n",
    "                                          drop_path_rate=args.drop_path_rate,\n",
    "                                          mode='train')\n",
    "            else:\n",
    "                model = ImageModel(model_name=args.model,\n",
    "                                   class_n=len(label_encoder.keys()),\n",
    "                                   drop_path_rate=args.drop_path_rate,\n",
    "                                   mode='train')\n",
    "\n",
    "        model = nn.DataParallel(model.cuda())\n",
    "\n",
    "        # Dataset\n",
    "        train_dataset = globals()[args.dataset](args.image_size, train_data, csv_files, avail_features, label_description, args.aug_ver, mode='train')\n",
    "        val_dataset = globals()[args.dataset](args.image_size, val_data, csv_files, avail_features, label_description, mode='valid')\n",
    "\n",
    "\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                                   num_workers=args.num_workers, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,\n",
    "                                                 num_workers=args.num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "        # Optimizer & Scheduler Setting\n",
    "        optimizer = None\n",
    "        if args.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                   lr=args.learning_rate,\n",
    "                                   weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=args.learning_rate,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'radam':\n",
    "            optimizer = optim.RAdam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=args.learning_rate,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'ranger':\n",
    "            optimizer = optim.Ranger(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=args.learning_rate,\n",
    "                                     betas=(0.9, 0.999),\n",
    "                                     weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'lamb':\n",
    "            optimizer = optim.Lamb(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                   lr=args.learning_rate,\n",
    "                                   weight_decay=args.weight_decay)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        iter_per_epoch = len(train_loader)\n",
    "        warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * args.warm_epoch)\n",
    "        scheduler = None\n",
    "        if args.scheduler == 'cos_base':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "        elif args.scheduler == 'cos_warm_restart':\n",
    "            args.epochs = 69\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, last_epoch=-1)\n",
    "        elif args.scheduler == 'cos':\n",
    "             # tmax = epoch * 2 => half-cycle\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.tmax, eta_min=args.min_lr)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "\n",
    "        # amp scaler\n",
    "        scaler = None\n",
    "        if args.amp:\n",
    "            scaler = GradScaler()\n",
    "\n",
    "        # log dir setting\n",
    "        log_dir = init_logger(args.save_path, args.comment)\n",
    "        args.log_dir = log_dir\n",
    "\n",
    "        # Wandb initialization\n",
    "        run = None\n",
    "        if args.wandb:\n",
    "            c_date, c_time = datetime.now().strftime(\"%m%d/%H%M%S\").split('/')\n",
    "            run = wandb.init(project=args.dataset, name=f'{args.model}_{c_date}_{c_time}_fold_{fold}')\n",
    "            wandb.config.update(args)\n",
    "\n",
    "        best_vacc, best_f1 = .0, .0\n",
    "        best_vloss = 9999.\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train(model, train_loader, criterion, optimizer, warmup_scheduler, scheduler, scaler, epoch, wandb, args)\n",
    "            vloss, vacc, vf1 = valid(model, val_loader, criterion, epoch, wandb, args)\n",
    "            predict_label(model, val_loader, label_description, label_decoder, epoch, fold, args)\n",
    "            if vf1 > best_f1:\n",
    "                best_epoch = epoch\n",
    "                best_vloss = min(vloss, best_vloss)\n",
    "                best_f1 = max(vf1, best_f1)\n",
    "                if best_f1 > 0.9:\n",
    "                    torch.save({'model_state_dict': model.module.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler': scheduler.state_dict(),\n",
    "                                'epoch': epoch, },\n",
    "                               f'{log_dir}/ckpts/ckpt_epoch_{epoch:03d}_fold_{fold:01d}.pt')\n",
    "\n",
    "            if args.scheduler in ['cos_base', 'cos', 'cos_warm_restart']:\n",
    "                if epoch > args.warm_epoch:\n",
    "                    scheduler.step()\n",
    "\n",
    "        best_model_paths.append(f'{log_dir}/ckpts/ckpt_epoch_{best_epoch:03d}_fold_{fold:01d}.pt')\n",
    "\n",
    "        del model\n",
    "        del optimizer, scheduler\n",
    "        del train_dataset, val_dataset\n",
    "\n",
    "        ### Best Model Inference\n",
    "        # Multi-modal Model\n",
    "        if args.environment_feature:\n",
    "            model = ImageModel2LSTMModel(model_name=args.model,\n",
    "                                         pretrained_model_path=best_image_model_paths[fold],\n",
    "                                         max_len=args.max_len,\n",
    "                                         img_embedding_dim=args.img_embedding_dim,\n",
    "                                         env_embedding_dim=args.env_embedding_dim,\n",
    "                                         num_features=len(avail_features),\n",
    "                                         class_n=len(label_encoder.keys()),\n",
    "                                         dropout_rate=0,\n",
    "                                         mode='test')\n",
    "\n",
    "        # Only-image Model\n",
    "        else:\n",
    "            if 'arc' in args.model :\n",
    "                model = ArcfaceImageModel(model_name=args.model,\n",
    "                                          class_n=len(label_encoder.keys()),\n",
    "                                          drop_path_rate=0,\n",
    "                                          mode='test')\n",
    "            else:\n",
    "                model = ImageModel(model_name=args.model,\n",
    "                                   class_n=len(label_encoder.keys()),\n",
    "                                   drop_path_rate=0,\n",
    "                                   mode='test')\n",
    "        model.load_state_dict(torch.load(f'{log_dir}/ckpts/ckpt_epoch_{best_epoch:03d}_fold_{fold:01d}.pt')['model_state_dict'])\n",
    "        model = nn.DataParallel(model.cuda())\n",
    "\n",
    "        test_dataset = globals()[args.dataset](args.image_size, test_data, csv_files, avail_features,\n",
    "                                               label_description, mode='test')\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                                  num_workers=args.num_workers, shuffle=False)\n",
    "        output_path = test(model, test_loader, label_decoder, best_epoch, fold, wandb, args)\n",
    "        output_path_list.append(output_path)\n",
    "        del test_dataset\n",
    "        del model\n",
    "\n",
    "        if args.wandb:\n",
    "            run.finish()\n",
    "\n",
    "    if args.data_split.lower() in ['stratifiedkfold','kfold'] :\n",
    "        ensemble_5fold_pt(model_name=args.model, output_path_list=output_path_list, label_decoder=label_decoder, args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datasets separated : 5\n",
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 284.64it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 285.26it/s]\n",
      "[EPOCH 1] Train Loss : 0.3845, Train Acc : 0.7628, Train F1 : 0.5597, LR : 1.00e-04: 100%|██████████████████████████| 165/165 [03:00<00:00,  1.09s/it]\n",
      "[EPOCH 1] Valid Loss : 0.0163, Valid Acc : 0.9118, Valid F1 : 0.7619: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Training:   0%|                                                                                                               | 0/165 [00:00<?, ?it/s]             [W accumulate_grad.h:184] Warning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 1, 7, 7], strides() = [49, 49, 7, 1]\n",
      "param.sizes() = [256, 1, 7, 7], strides() = [49, 1, 7, 1] (function operator())\n",
      "[EPOCH 2] Train Loss : 0.0536, Train Acc : 0.8773, Train F1 : 0.7173, LR : 9.94e-05: 100%|██████████████████████████| 165/165 [03:09<00:00,  1.15s/it]\n",
      "[EPOCH 2] Valid Loss : 0.0075, Valid Acc : 0.9396, Valid F1 : 0.8117: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.76it/s]\n",
      "[EPOCH 3] Train Loss : 0.0369, Train Acc : 0.8966, Train F1 : 0.7521, LR : 9.76e-05: 100%|██████████████████████████| 165/165 [03:14<00:00,  1.18s/it]             \n",
      "[EPOCH 3] Valid Loss : 0.0026, Valid Acc : 0.9558, Valid F1 : 0.8601: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.75it/s]\n",
      "[EPOCH 4] Train Loss : 0.0303, Train Acc : 0.9029, Train F1 : 0.7707, LR : 9.46e-05: 100%|██████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 4] Valid Loss : 0.0348, Valid Acc : 0.8886, Valid F1 : 0.7777: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 5] Train Loss : 0.0210, Train Acc : 0.9131, Train F1 : 0.8076, LR : 9.05e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 5] Valid Loss : 0.0040, Valid Acc : 0.9595, Valid F1 : 0.8783: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 6] Train Loss : 0.0188, Train Acc : 0.9186, Train F1 : 0.8082, LR : 8.54e-05: 100%|██████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 6] Valid Loss : 0.0032, Valid Acc : 0.9575, Valid F1 : 0.8716: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "[EPOCH 7] Train Loss : 0.0129, Train Acc : 0.9245, Train F1 : 0.8309, LR : 7.94e-05: 100%|██████████████████████████| 165/165 [03:27<00:00,  1.26s/it]             \n",
      "[EPOCH 7] Valid Loss : 0.0014, Valid Acc : 0.9753, Valid F1 : 0.9226: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.75it/s]\n",
      "[EPOCH 8] Train Loss : 0.0092, Train Acc : 0.9333, Train F1 : 0.8516, LR : 7.27e-05: 100%|██████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 8] Valid Loss : 0.0010, Valid Acc : 0.9753, Valid F1 : 0.9201: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 9] Train Loss : 0.0102, Train Acc : 0.9298, Train F1 : 0.8512, LR : 6.55e-05: 100%|██████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 9] Valid Loss : 0.0015, Valid Acc : 0.9736, Valid F1 : 0.9135: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 10] Train Loss : 0.0088, Train Acc : 0.9346, Train F1 : 0.8614, LR : 5.78e-05: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 10] Valid Loss : 0.0010, Valid Acc : 0.9779, Valid F1 : 0.9313: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 11] Train Loss : 0.0075, Train Acc : 0.9379, Train F1 : 0.8732, LR : 5.00e-05: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.24s/it]             \n",
      "[EPOCH 11] Valid Loss : 0.0008, Valid Acc : 0.9830, Valid F1 : 0.9476: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.65it/s]\n",
      "[EPOCH 12] Train Loss : 0.0067, Train Acc : 0.9430, Train F1 : 0.8761, LR : 4.22e-05: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.22s/it]             \n",
      "[EPOCH 12] Valid Loss : 0.0010, Valid Acc : 0.9779, Valid F1 : 0.9254: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "[EPOCH 13] Train Loss : 0.0051, Train Acc : 0.9415, Train F1 : 0.8749, LR : 3.45e-05: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 13] Valid Loss : 0.0010, Valid Acc : 0.9787, Valid F1 : 0.9303: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 14] Train Loss : 0.0048, Train Acc : 0.9506, Train F1 : 0.8977, LR : 2.73e-05: 100%|█████████████████████████| 165/165 [03:25<00:00,  1.24s/it]             \n",
      "[EPOCH 14] Valid Loss : 0.0010, Valid Acc : 0.9830, Valid F1 : 0.9413: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 15] Train Loss : 0.0061, Train Acc : 0.9406, Train F1 : 0.8843, LR : 2.06e-05: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 15] Valid Loss : 0.0016, Valid Acc : 0.9796, Valid F1 : 0.9316: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.67it/s]\n",
      "[EPOCH 16] Train Loss : 0.0052, Train Acc : 0.9478, Train F1 : 0.8909, LR : 1.46e-05: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.24s/it]             \n",
      "[EPOCH 16] Valid Loss : 0.0015, Valid Acc : 0.9855, Valid F1 : 0.9468: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 17] Train Loss : 0.0057, Train Acc : 0.9426, Train F1 : 0.8827, LR : 9.55e-06: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 17] Valid Loss : 0.0020, Valid Acc : 0.9796, Valid F1 : 0.9312: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.68it/s]\n",
      "[EPOCH 18] Train Loss : 0.0039, Train Acc : 0.9530, Train F1 : 0.9016, LR : 5.45e-06: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 18] Valid Loss : 0.0013, Valid Acc : 0.9847, Valid F1 : 0.9417: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.63it/s]\n",
      "[EPOCH 19] Train Loss : 0.0045, Train Acc : 0.9475, Train F1 : 0.8926, LR : 2.45e-06: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 19] Valid Loss : 0.0014, Valid Acc : 0.9821, Valid F1 : 0.9389: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.66it/s]\n",
      "[EPOCH 20] Train Loss : 0.0047, Train Acc : 0.9427, Train F1 : 0.8757, LR : 6.16e-07: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 20] Valid Loss : 0.0014, Valid Acc : 0.9830, Valid F1 : 0.9425: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:19<00:00, 289.23it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 281.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 283.51it/s]\n",
      "[EPOCH 1] Train Loss : 0.3689, Train Acc : 0.7656, Train F1 : 0.5644, LR : 1.00e-04: 100%|██████████████████████████| 165/165 [03:09<00:00,  1.15s/it]\n",
      "[EPOCH 1] Valid Loss : 0.0189, Valid Acc : 0.9167, Valid F1 : 0.7993: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 2] Train Loss : 0.0587, Train Acc : 0.8798, Train F1 : 0.7232, LR : 9.94e-05: 100%|██████████████████████████| 165/165 [03:23<00:00,  1.24s/it]             \n",
      "[EPOCH 2] Valid Loss : 0.0047, Valid Acc : 0.9549, Valid F1 : 0.8717: 100%|███████████████████████████████████████████| 42/42 [00:25<00:00,  1.65it/s]\n",
      "[EPOCH 3] Train Loss : 0.0362, Train Acc : 0.8956, Train F1 : 0.7605, LR : 9.76e-05: 100%|██████████████████████████| 165/165 [03:25<00:00,  1.25s/it]             \n",
      "[EPOCH 3] Valid Loss : 0.0141, Valid Acc : 0.9246, Valid F1 : 0.8075: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 4] Train Loss : 0.0263, Train Acc : 0.9030, Train F1 : 0.7841, LR : 9.46e-05: 100%|██████████████████████████| 165/165 [03:26<00:00,  1.25s/it]             \n",
      "[EPOCH 4] Valid Loss : 0.0036, Valid Acc : 0.9569, Valid F1 : 0.8798: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 5] Train Loss : 0.0173, Train Acc : 0.9197, Train F1 : 0.8145, LR : 9.05e-05: 100%|██████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 5] Valid Loss : 0.0021, Valid Acc : 0.9697, Valid F1 : 0.9193: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 6] Train Loss : 0.0177, Train Acc : 0.9203, Train F1 : 0.8180, LR : 8.54e-05: 100%|██████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 6] Valid Loss : 0.0019, Valid Acc : 0.9711, Valid F1 : 0.9266: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "[EPOCH 7] Train Loss : 0.0123, Train Acc : 0.9239, Train F1 : 0.8308, LR : 7.94e-05: 100%|██████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 7] Valid Loss : 0.0035, Valid Acc : 0.9702, Valid F1 : 0.9258: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 8] Train Loss : 0.0099, Train Acc : 0.9308, Train F1 : 0.8419, LR : 7.27e-05: 100%|██████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 8] Valid Loss : 0.0017, Valid Acc : 0.9719, Valid F1 : 0.9191: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 9] Train Loss : 0.0130, Train Acc : 0.9245, Train F1 : 0.8399, LR : 6.55e-05: 100%|██████████████████████████| 165/165 [03:25<00:00,  1.25s/it]             \n",
      "[EPOCH 9] Valid Loss : 0.0021, Valid Acc : 0.9770, Valid F1 : 0.9347: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 10] Train Loss : 0.0082, Train Acc : 0.9396, Train F1 : 0.8760, LR : 5.78e-05: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 10] Valid Loss : 0.0038, Valid Acc : 0.9804, Valid F1 : 0.9514: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "[EPOCH 11] Train Loss : 0.0070, Train Acc : 0.9421, Train F1 : 0.8789, LR : 5.00e-05: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 11] Valid Loss : 0.0027, Valid Acc : 0.9838, Valid F1 : 0.9586: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 12] Train Loss : 0.0073, Train Acc : 0.9398, Train F1 : 0.8708, LR : 4.22e-05: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 12] Valid Loss : 0.0024, Valid Acc : 0.9821, Valid F1 : 0.9523: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 13] Train Loss : 0.0061, Train Acc : 0.9457, Train F1 : 0.8853, LR : 3.45e-05: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 13] Valid Loss : 0.0032, Valid Acc : 0.9779, Valid F1 : 0.9376: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 14] Train Loss : 0.0051, Train Acc : 0.9466, Train F1 : 0.8894, LR : 2.73e-05: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 14] Valid Loss : 0.0041, Valid Acc : 0.9804, Valid F1 : 0.9468: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.67it/s]\n",
      "[EPOCH 15] Train Loss : 0.0051, Train Acc : 0.9477, Train F1 : 0.8931, LR : 2.06e-05: 100%|█████████████████████████| 165/165 [03:25<00:00,  1.25s/it]             \n",
      "[EPOCH 15] Valid Loss : 0.0052, Valid Acc : 0.9753, Valid F1 : 0.9393: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 16] Train Loss : 0.0051, Train Acc : 0.9454, Train F1 : 0.8870, LR : 1.46e-05: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.24s/it]             \n",
      "[EPOCH 16] Valid Loss : 0.0040, Valid Acc : 0.9821, Valid F1 : 0.9535: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 17] Train Loss : 0.0043, Train Acc : 0.9555, Train F1 : 0.9094, LR : 9.55e-06: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 17] Valid Loss : 0.0035, Valid Acc : 0.9821, Valid F1 : 0.9502: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.67it/s]\n",
      "[EPOCH 18] Train Loss : 0.0034, Train Acc : 0.9491, Train F1 : 0.9009, LR : 5.45e-06: 100%|█████████████████████████| 165/165 [03:24<00:00,  1.24s/it]             \n",
      "[EPOCH 18] Valid Loss : 0.0042, Valid Acc : 0.9838, Valid F1 : 0.9504: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 19] Train Loss : 0.0045, Train Acc : 0.9515, Train F1 : 0.8949, LR : 2.45e-06: 100%|█████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 19] Valid Loss : 0.0032, Valid Acc : 0.9830, Valid F1 : 0.9561: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 20] Train Loss : 0.0043, Train Acc : 0.9485, Train F1 : 0.8987, LR : 6.16e-07: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 20] Valid Loss : 0.0044, Valid Acc : 0.9821, Valid F1 : 0.9553: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 281.77it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 280.92it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 282.98it/s]\n",
      "[EPOCH 1] Train Loss : 0.4016, Train Acc : 0.7609, Train F1 : 0.5445, LR : 1.00e-04: 100%|██████████████████████████| 165/165 [03:09<00:00,  1.15s/it]\n",
      "[EPOCH 1] Valid Loss : 0.0163, Valid Acc : 0.9153, Valid F1 : 0.7767: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 2] Train Loss : 0.0525, Train Acc : 0.8881, Train F1 : 0.7341, LR : 9.94e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 2] Valid Loss : 0.0045, Valid Acc : 0.9383, Valid F1 : 0.8316: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 3] Train Loss : 0.0326, Train Acc : 0.8964, Train F1 : 0.7637, LR : 9.76e-05: 100%|██████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 3] Valid Loss : 0.0024, Valid Acc : 0.9527, Valid F1 : 0.8683: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "[EPOCH 4] Train Loss : 0.0246, Train Acc : 0.9099, Train F1 : 0.7987, LR : 9.46e-05: 100%|██████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 4] Valid Loss : 0.0021, Valid Acc : 0.9578, Valid F1 : 0.8784: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 5] Train Loss : 0.0173, Train Acc : 0.9206, Train F1 : 0.8254, LR : 9.05e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 5] Valid Loss : 0.0032, Valid Acc : 0.9442, Valid F1 : 0.8494: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 6] Train Loss : 0.0128, Train Acc : 0.9280, Train F1 : 0.8316, LR : 8.54e-05: 100%|██████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 6] Valid Loss : 0.0042, Valid Acc : 0.9629, Valid F1 : 0.8949: 100%|███████████████████████████████████████████| 42/42 [00:25<00:00,  1.68it/s]\n",
      "[EPOCH 7] Train Loss : 0.0137, Train Acc : 0.9228, Train F1 : 0.8325, LR : 7.94e-05: 100%|██████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 7] Valid Loss : 0.0038, Valid Acc : 0.9672, Valid F1 : 0.9032: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 8] Train Loss : 0.0115, Train Acc : 0.9290, Train F1 : 0.8450, LR : 7.27e-05: 100%|██████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 8] Valid Loss : 0.0107, Valid Acc : 0.9638, Valid F1 : 0.9031: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.75it/s]\n",
      "[EPOCH 9] Train Loss : 0.0094, Train Acc : 0.9292, Train F1 : 0.8458, LR : 6.55e-05: 100%|██████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 9] Valid Loss : 0.0093, Valid Acc : 0.9680, Valid F1 : 0.9071: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 10] Train Loss : 0.0071, Train Acc : 0.9391, Train F1 : 0.8650, LR : 5.78e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.22s/it]             \n",
      "[EPOCH 10] Valid Loss : 0.0087, Valid Acc : 0.9689, Valid F1 : 0.9095: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 11] Train Loss : 0.0057, Train Acc : 0.9452, Train F1 : 0.8802, LR : 5.00e-05: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 11] Valid Loss : 0.0086, Valid Acc : 0.9655, Valid F1 : 0.9027: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.68it/s]\n",
      "[EPOCH 12] Train Loss : 0.0057, Train Acc : 0.9457, Train F1 : 0.8855, LR : 4.22e-05: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 12] Valid Loss : 0.0121, Valid Acc : 0.9689, Valid F1 : 0.9074: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.70it/s]\n",
      "[EPOCH 13] Train Loss : 0.0055, Train Acc : 0.9472, Train F1 : 0.8863, LR : 3.45e-05: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.22s/it]             \n",
      "[EPOCH 13] Valid Loss : 0.0078, Valid Acc : 0.9697, Valid F1 : 0.9133: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 14] Train Loss : 0.0048, Train Acc : 0.9473, Train F1 : 0.8920, LR : 2.73e-05: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 14] Valid Loss : 0.0169, Valid Acc : 0.9706, Valid F1 : 0.9175: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 15] Train Loss : 0.0047, Train Acc : 0.9447, Train F1 : 0.8952, LR : 2.06e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 15] Valid Loss : 0.0154, Valid Acc : 0.9731, Valid F1 : 0.9262: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 16] Train Loss : 0.0037, Train Acc : 0.9538, Train F1 : 0.9084, LR : 1.46e-05: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 16] Valid Loss : 0.0136, Valid Acc : 0.9740, Valid F1 : 0.9265: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 17] Train Loss : 0.0045, Train Acc : 0.9506, Train F1 : 0.8999, LR : 9.55e-06: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 17] Valid Loss : 0.0150, Valid Acc : 0.9748, Valid F1 : 0.9299: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.68it/s]\n",
      "[EPOCH 18] Train Loss : 0.0043, Train Acc : 0.9498, Train F1 : 0.8966, LR : 5.45e-06: 100%|█████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 18] Valid Loss : 0.0190, Valid Acc : 0.9714, Valid F1 : 0.9206: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.66it/s]\n",
      "[EPOCH 19] Train Loss : 0.0047, Train Acc : 0.9466, Train F1 : 0.8902, LR : 2.45e-06: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 19] Valid Loss : 0.0118, Valid Acc : 0.9731, Valid F1 : 0.9271: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.65it/s]\n",
      "[EPOCH 20] Train Loss : 0.0039, Train Acc : 0.9490, Train F1 : 0.8977, LR : 6.16e-07: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.22s/it]             \n",
      "[EPOCH 20] Valid Loss : 0.0168, Valid Acc : 0.9723, Valid F1 : 0.9238: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 280.71it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 288.22it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:19<00:00, 290.19it/s]\n",
      "[EPOCH 1] Train Loss : 0.3977, Train Acc : 0.7535, Train F1 : 0.5416, LR : 1.00e-04: 100%|██████████████████████████| 165/165 [03:09<00:00,  1.15s/it]\n",
      "[EPOCH 1] Valid Loss : 0.0313, Valid Acc : 0.8988, Valid F1 : 0.7574: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 2] Train Loss : 0.0511, Train Acc : 0.8858, Train F1 : 0.7309, LR : 9.94e-05: 100%|██████████████████████████| 165/165 [03:20<00:00,  1.22s/it]             \n",
      "[EPOCH 2] Valid Loss : 0.0136, Valid Acc : 0.9269, Valid F1 : 0.7875: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 3] Train Loss : 0.0336, Train Acc : 0.8982, Train F1 : 0.7589, LR : 9.76e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 3] Valid Loss : 0.0037, Valid Acc : 0.9524, Valid F1 : 0.8560: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 4] Train Loss : 0.0244, Train Acc : 0.9178, Train F1 : 0.8175, LR : 9.46e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 4] Valid Loss : 0.0123, Valid Acc : 0.9260, Valid F1 : 0.8226: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.76it/s]\n",
      "[EPOCH 5] Train Loss : 0.0170, Train Acc : 0.9214, Train F1 : 0.8169, LR : 9.05e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 5] Valid Loss : 0.0065, Valid Acc : 0.9507, Valid F1 : 0.8552: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 6] Train Loss : 0.0116, Train Acc : 0.9341, Train F1 : 0.8517, LR : 8.54e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 6] Valid Loss : 0.0059, Valid Acc : 0.9634, Valid F1 : 0.8861: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 7] Train Loss : 0.0117, Train Acc : 0.9264, Train F1 : 0.8332, LR : 7.94e-05: 100%|██████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 7] Valid Loss : 0.0052, Valid Acc : 0.9600, Valid F1 : 0.8825: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 8] Train Loss : 0.0092, Train Acc : 0.9374, Train F1 : 0.8584, LR : 7.27e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 8] Valid Loss : 0.0052, Valid Acc : 0.9617, Valid F1 : 0.8848: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 9] Train Loss : 0.0112, Train Acc : 0.9323, Train F1 : 0.8550, LR : 6.55e-05: 100%|██████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 9] Valid Loss : 0.0035, Valid Acc : 0.9651, Valid F1 : 0.8903: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 10] Train Loss : 0.0089, Train Acc : 0.9354, Train F1 : 0.8551, LR : 5.78e-05: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 10] Valid Loss : 0.0042, Valid Acc : 0.9643, Valid F1 : 0.8943: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 11] Train Loss : 0.0060, Train Acc : 0.9418, Train F1 : 0.8767, LR : 5.00e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.22s/it]             \n",
      "[EPOCH 11] Valid Loss : 0.0055, Valid Acc : 0.9677, Valid F1 : 0.8947: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 12] Train Loss : 0.0065, Train Acc : 0.9423, Train F1 : 0.8731, LR : 4.22e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.22s/it]             \n",
      "[EPOCH 12] Valid Loss : 0.0049, Valid Acc : 0.9668, Valid F1 : 0.9063: 100%|██████████████████████████████████████████| 42/42 [00:26<00:00,  1.61it/s]\n",
      "[EPOCH 13] Train Loss : 0.0051, Train Acc : 0.9497, Train F1 : 0.8870, LR : 3.45e-05: 100%|█████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 13] Valid Loss : 0.0059, Valid Acc : 0.9677, Valid F1 : 0.8937: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 14] Train Loss : 0.0049, Train Acc : 0.9505, Train F1 : 0.8993, LR : 2.73e-05: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 14] Valid Loss : 0.0054, Valid Acc : 0.9668, Valid F1 : 0.8959: 100%|██████████████████████████████████████████| 42/42 [00:25<00:00,  1.64it/s]\n",
      "[EPOCH 15] Train Loss : 0.0042, Train Acc : 0.9494, Train F1 : 0.8960, LR : 2.06e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 15] Valid Loss : 0.0050, Valid Acc : 0.9660, Valid F1 : 0.8952: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 16] Train Loss : 0.0052, Train Acc : 0.9451, Train F1 : 0.8814, LR : 1.46e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.22s/it]             \n",
      "[EPOCH 16] Valid Loss : 0.0054, Valid Acc : 0.9668, Valid F1 : 0.8984: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.69it/s]\n",
      "[EPOCH 17] Train Loss : 0.0050, Train Acc : 0.9447, Train F1 : 0.8859, LR : 9.55e-06: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 17] Valid Loss : 0.0047, Valid Acc : 0.9677, Valid F1 : 0.8967: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 18] Train Loss : 0.0040, Train Acc : 0.9489, Train F1 : 0.8913, LR : 5.45e-06: 100%|█████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 18] Valid Loss : 0.0059, Valid Acc : 0.9702, Valid F1 : 0.9033: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 19] Train Loss : 0.0048, Train Acc : 0.9502, Train F1 : 0.9045, LR : 2.45e-06: 100%|█████████████████████████| 165/165 [03:21<00:00,  1.22s/it]             \n",
      "[EPOCH 19] Valid Loss : 0.0055, Valid Acc : 0.9668, Valid F1 : 0.8955: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 20] Train Loss : 0.0042, Train Acc : 0.9477, Train F1 : 0.8943, LR : 6.16e-07: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 20] Valid Loss : 0.0059, Valid Acc : 0.9677, Valid F1 : 0.8992: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:19<00:00, 288.78it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 283.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 283.50it/s]\n",
      "[EPOCH 1] Train Loss : 0.4009, Train Acc : 0.7632, Train F1 : 0.5643, LR : 1.00e-04: 100%|██████████████████████████| 165/165 [03:09<00:00,  1.15s/it]\n",
      "[EPOCH 1] Valid Loss : 0.0159, Valid Acc : 0.9141, Valid F1 : 0.7669: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 2] Train Loss : 0.0577, Train Acc : 0.8798, Train F1 : 0.7270, LR : 9.94e-05: 100%|██████████████████████████| 165/165 [03:18<00:00,  1.20s/it]             \n",
      "[EPOCH 2] Valid Loss : 0.0089, Valid Acc : 0.9328, Valid F1 : 0.8021: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 3] Train Loss : 0.0339, Train Acc : 0.8992, Train F1 : 0.7617, LR : 9.76e-05: 100%|██████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 3] Valid Loss : 0.0047, Valid Acc : 0.9481, Valid F1 : 0.8331: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 4] Train Loss : 0.0240, Train Acc : 0.9075, Train F1 : 0.7911, LR : 9.46e-05: 100%|██████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 4] Valid Loss : 0.0024, Valid Acc : 0.9515, Valid F1 : 0.8545: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 5] Train Loss : 0.0211, Train Acc : 0.9151, Train F1 : 0.8032, LR : 9.05e-05: 100%|██████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 5] Valid Loss : 0.0012, Valid Acc : 0.9668, Valid F1 : 0.8904: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 6] Train Loss : 0.0148, Train Acc : 0.9251, Train F1 : 0.8316, LR : 8.54e-05: 100%|██████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 6] Valid Loss : 0.0016, Valid Acc : 0.9694, Valid F1 : 0.8943: 100%|███████████████████████████████████████████| 42/42 [00:25<00:00,  1.66it/s]\n",
      "[EPOCH 7] Train Loss : 0.0140, Train Acc : 0.9247, Train F1 : 0.8242, LR : 7.94e-05: 100%|██████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 7] Valid Loss : 0.0014, Valid Acc : 0.9651, Valid F1 : 0.8912: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 8] Train Loss : 0.0125, Train Acc : 0.9282, Train F1 : 0.8457, LR : 7.27e-05: 100%|██████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 8] Valid Loss : 0.0026, Valid Acc : 0.9685, Valid F1 : 0.8951: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 9] Train Loss : 0.0109, Train Acc : 0.9312, Train F1 : 0.8510, LR : 6.55e-05: 100%|██████████████████████████| 165/165 [03:18<00:00,  1.20s/it]             \n",
      "[EPOCH 9] Valid Loss : 0.0016, Valid Acc : 0.9668, Valid F1 : 0.8912: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 10] Train Loss : 0.0086, Train Acc : 0.9339, Train F1 : 0.8536, LR : 5.78e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 10] Valid Loss : 0.0034, Valid Acc : 0.9685, Valid F1 : 0.8943: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 11] Train Loss : 0.0054, Train Acc : 0.9456, Train F1 : 0.8870, LR : 5.00e-05: 100%|█████████████████████████| 165/165 [03:20<00:00,  1.21s/it]             \n",
      "[EPOCH 11] Valid Loss : 0.0018, Valid Acc : 0.9728, Valid F1 : 0.9071: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 12] Train Loss : 0.0056, Train Acc : 0.9469, Train F1 : 0.8824, LR : 4.22e-05: 100%|█████████████████████████| 165/165 [03:18<00:00,  1.20s/it]             \n",
      "[EPOCH 12] Valid Loss : 0.0017, Valid Acc : 0.9728, Valid F1 : 0.9041: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 13] Train Loss : 0.0045, Train Acc : 0.9480, Train F1 : 0.8908, LR : 3.45e-05: 100%|█████████████████████████| 165/165 [03:18<00:00,  1.21s/it]             \n",
      "[EPOCH 13] Valid Loss : 0.0018, Valid Acc : 0.9711, Valid F1 : 0.8947: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "[EPOCH 14] Train Loss : 0.0048, Train Acc : 0.9438, Train F1 : 0.8832, LR : 2.73e-05: 100%|█████████████████████████| 165/165 [03:18<00:00,  1.20s/it]             \n",
      "[EPOCH 14] Valid Loss : 0.0020, Valid Acc : 0.9736, Valid F1 : 0.9054: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 15] Train Loss : 0.0051, Train Acc : 0.9459, Train F1 : 0.8931, LR : 2.06e-05: 100%|█████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 15] Valid Loss : 0.0044, Valid Acc : 0.9668, Valid F1 : 0.8986: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "[EPOCH 16] Train Loss : 0.0045, Train Acc : 0.9494, Train F1 : 0.9014, LR : 1.46e-05: 100%|█████████████████████████| 165/165 [03:18<00:00,  1.20s/it]             \n",
      "[EPOCH 16] Valid Loss : 0.0019, Valid Acc : 0.9753, Valid F1 : 0.9095: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 17] Train Loss : 0.0043, Train Acc : 0.9497, Train F1 : 0.8914, LR : 9.55e-06: 100%|█████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 17] Valid Loss : 0.0022, Valid Acc : 0.9728, Valid F1 : 0.8985: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 18] Train Loss : 0.0045, Train Acc : 0.9468, Train F1 : 0.8882, LR : 5.45e-06: 100%|█████████████████████████| 165/165 [03:19<00:00,  1.21s/it]             \n",
      "[EPOCH 18] Valid Loss : 0.0025, Valid Acc : 0.9770, Valid F1 : 0.9181: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 19] Train Loss : 0.0040, Train Acc : 0.9479, Train F1 : 0.8890, LR : 2.45e-06: 100%|█████████████████████████| 165/165 [03:18<00:00,  1.21s/it]             \n",
      "[EPOCH 19] Valid Loss : 0.0023, Valid Acc : 0.9745, Valid F1 : 0.9054: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 20] Train Loss : 0.0052, Train Acc : 0.9475, Train F1 : 0.8984, LR : 6.16e-07: 100%|█████████████████████████| 165/165 [03:18<00:00,  1.21s/it]             \n",
      "[EPOCH 20] Valid Loss : 0.0030, Valid Acc : 0.9745, Valid F1 : 0.9059: 100%|██████████████████████████████████████████| 42/42 [00:24<00:00,  1.73it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5766/5766 [00:20<00:00, 286.98it/s]\n",
      "                                                                                                                                                                   \r"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-plant",
   "language": "python",
   "name": "python3-plant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
