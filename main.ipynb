{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "import argparse\n",
    "\n",
    "from dataset import PlantDACON\n",
    "\n",
    "import timm\n",
    "import torch_optimizer as optim\n",
    "\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.cuda.amp import GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from collections import OrderedDict\n",
    "import timm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import albumentations as albu\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "class PlantDACON(Dataset):\n",
    "    def __init__(self,\n",
    "                 image_size,\n",
    "                 files,\n",
    "                 csv_files=None,\n",
    "                 avail_features=None,\n",
    "                 label_description=None,\n",
    "                 aug_ver=0,\n",
    "                 tta_transform=None,\n",
    "                 mode='train'):\n",
    "        self.mode = mode\n",
    "        self.image_size = image_size\n",
    "        self.files = files\n",
    "        self.aug_ver = aug_ver\n",
    "        self.avail_features = avail_features\n",
    "        self.csv_files = csv_files\n",
    "        self.csv_feature_check = [0] * len(self.files)\n",
    "        self.csv_features = [None] * len(self.files)\n",
    "        self.max_len = 320\n",
    "        self.label_description = label_description\n",
    "        self.label_encoder = {key: idx for idx, key in enumerate(self.label_description)}\n",
    "        self.base_transform_list = [\n",
    "            albu.Resize(self.image_size, self.image_size),\n",
    "            albu.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1,\n",
    "                                  rotate_limit=30, interpolation=1, border_mode=0,\n",
    "                                  value=0, p=0.5),\n",
    "            albu.HorizontalFlip(p=0.5),\n",
    "            albu.VerticalFlip(p=0.5)\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.tta_transform = tta_transform\n",
    "        self.transform_normalize = [\n",
    "            albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2()\n",
    "        ]\n",
    "        if self.mode == 'train':\n",
    "            # Only-flip\n",
    "            if self.aug_ver == 1:\n",
    "                self.transform = albu.Compose([albu.Resize(self.image_size, self.image_size),\n",
    "                                               albu.HorizontalFlip(p=0.5),\n",
    "                                               albu.VerticalFlip(p=0.5),\n",
    "                                               albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                                               ToTensorV2(),\n",
    "                                               ])\n",
    "            # Base Augmentation\n",
    "            elif self.aug_ver == 2:\n",
    "                self.transform = albu.Compose(\n",
    "                    self.base_transform_list\n",
    "                    + self.transform_normalize)\n",
    "\n",
    "            elif self.aug_ver == 24:\n",
    "                self.transform = albu.Compose(\n",
    "                    self.base_transform_list +\n",
    "                    [\n",
    "                        albu.CLAHE(clip_limit=2, p=0.25),\n",
    "                        albu.Sharpen(p=0.25),\n",
    "                        albu.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1),\n",
    "                                                      contrast_limit=(-0.1, 0.1), p=0.25),\n",
    "                        albu.RandomResizedCrop(height=self.image_size, width=self.image_size,\n",
    "                                               scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333),\n",
    "                                               interpolation=1, p=1.0),\n",
    "                    ]\n",
    "                    + self.transform_normalize)\n",
    "            elif self.aug_ver == 29:\n",
    "                self.transform = albu.Compose(\n",
    "                    self.base_transform_list +\n",
    "                    [\n",
    "                        albu.RandomRotate90(p=1.0),\n",
    "                        albu.CLAHE(clip_limit=2, p=0.25),\n",
    "                        albu.Sharpen(p=0.25),\n",
    "                        albu.RandomBrightnessContrast(brightness_limit=(-0.1, 0.1),\n",
    "                                                      contrast_limit=(-0.1, 0.1), p=0.25),\n",
    "                        albu.RandomResizedCrop(height=self.image_size, width=self.image_size,\n",
    "                                               scale=(0.5, 1.0), ratio=(0.75, 1.3333333333333333),\n",
    "                                               interpolation=1, p=1.0),\n",
    "                    ]\n",
    "                    + self.transform_normalize)\n",
    "            else :\n",
    "                self.transform = albu.Compose([albu.Resize(self.image_size, self.image_size),\n",
    "                                               albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                                               ToTensorV2(),\n",
    "                                               ])\n",
    "        elif self.mode == 'valid':\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Resize(self.image_size, self.image_size),\n",
    "                albu.HorizontalFlip(p=0.5),\n",
    "                albu.VerticalFlip(p=0.5),\n",
    "                albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        elif self.mode == 'test':\n",
    "            if self.tta_transform:\n",
    "                self.transform = albu.Compose(\n",
    "                    [albu.Resize(self.image_size, self.image_size)]+\n",
    "                    [\n",
    "                        self.tta_transform\n",
    "                    ]\n",
    "                    + self.transform_normalize)\n",
    "            else:\n",
    "                self.transform = albu.Compose([\n",
    "                    albu.Resize(self.image_size, self.image_size),\n",
    "                    albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                    ToTensorV2(),\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = albu.Compose([\n",
    "                albu.Resize(self.image_size, self.image_size),\n",
    "                albu.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "\n",
    "        if avail_features:\n",
    "            # self.csv_feature_dict = self.make_csv_feature_dict()\n",
    "            self.csv_feature_dict = {'내부 온도 1 평균': [3.4, 47.3],\n",
    "                                     '내부 온도 1 최고': [3.4, 47.6],\n",
    "                                     '내부 온도 1 최저': [3.3, 47.0],\n",
    "                                     '내부 습도 1 평균': [23.7, 100.0],\n",
    "                                     '내부 습도 1 최고': [25.9, 100.0],\n",
    "                                     '내부 습도 1 최저': [0.0, 100.0],\n",
    "                                     '내부 이슬점 평균': [0.1, 34.5],\n",
    "                                     '내부 이슬점 최고': [0.2, 34.7],\n",
    "                                     '내부 이슬점 최저': [0.0, 34.4]}\n",
    "    def make_csv_feature_dict(self):\n",
    "        # 분석에 사용할 feature 선택\n",
    "        temp_csv = pd.read_csv(self.csv_files[0])[self.avail_features]\n",
    "        max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "\n",
    "        # feature 별 최대값, 최솟값 계산\n",
    "        for csv in tqdm(self.csv_files[1:]):\n",
    "            temp_csv = pd.read_csv(csv)[self.avail_features]\n",
    "            temp_csv = temp_csv.replace('-', np.nan).dropna()\n",
    "            if len(temp_csv) == 0:\n",
    "                continue\n",
    "            temp_csv = temp_csv.astype(float)\n",
    "            temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
    "            max_arr = np.max([max_arr, temp_max], axis=0)\n",
    "            min_arr = np.min([min_arr, temp_min], axis=0)\n",
    "\n",
    "        # feature 별 최대값, 최솟값 dictionary 생성\n",
    "        csv_feature_dict = {self.avail_features[i]: [min_arr[i], max_arr[i]] for i in range(len(self.avail_features))}\n",
    "        return csv_feature_dict\n",
    "\n",
    "    def get_csv_feature_dict(self):\n",
    "        return self.csv_feature_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        file = self.files[i]\n",
    "        file_name = file.split('/')[-1]\n",
    "\n",
    "        json_path = f'{file}/{file_name}.json'\n",
    "        image_path = f'{file}/{file_name}.jpg'\n",
    "        if self.avail_features:\n",
    "            if self.csv_feature_check[i] == 0:\n",
    "                csv_path = f'{file}/{file_name}.csv'\n",
    "                df = pd.read_csv(csv_path)[self.csv_feature_dict.keys()]\n",
    "                df = df.replace('-', 0)\n",
    "                # MinMax scaling\n",
    "                for col in df.columns:\n",
    "                    df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
    "                    df[col] = df[col] / (self.csv_feature_dict[col][1] - self.csv_feature_dict[col][0])\n",
    "                # zero padding\n",
    "                pad = np.zeros((self.max_len, len(df.columns)))\n",
    "                length = min(self.max_len, len(df))\n",
    "                pad[-length:] = df.to_numpy()[-length:]\n",
    "                # transpose to sequential data\n",
    "                csv_feature = pad.T\n",
    "                self.csv_features[i] = csv_feature\n",
    "                self.csv_feature_check[i] = 1\n",
    "            else:\n",
    "                csv_feature = self.csv_features[i]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        img = cv2.imread(image_path)\n",
    "        img = self.transform(image=img)\n",
    "\n",
    "        if self.avail_features:\n",
    "            if self.mode in ['train', 'valid']:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    json_file = json.load(f)\n",
    "\n",
    "                crop = json_file['annotations']['crop']\n",
    "                disease = json_file['annotations']['disease']\n",
    "                risk = json_file['annotations']['risk']\n",
    "                label = f'{crop}_{disease}_{risk}'\n",
    "                return {\n",
    "                    'img': img,\n",
    "                    'csv_feature': torch.tensor(csv_feature, dtype=torch.float32),\n",
    "                    'label': torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                return {\n",
    "                    'img': img,\n",
    "                    'csv_feature': torch.tensor(csv_feature, dtype=torch.float32)\n",
    "                }\n",
    "        else:\n",
    "            if self.mode in ['train', 'valid']:\n",
    "                with open(json_path, 'r') as f:\n",
    "                    json_file = json.load(f)\n",
    "\n",
    "                crop = json_file['annotations']['crop']\n",
    "                disease = json_file['annotations']['disease']\n",
    "                risk = json_file['annotations']['risk']\n",
    "                label = f'{crop}_{disease}_{risk}'\n",
    "\n",
    "                return {\n",
    "                    'img': img,\n",
    "                    'label': torch.tensor(self.label_encoder[label], dtype=torch.long)\n",
    "                }\n",
    "\n",
    "            else:\n",
    "                return {\n",
    "                    'img': img,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ImageModel(nn.Module):\n",
    "    def __init__(self, model_name, class_n, drop_path_rate, mode='train'):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name.lower()\n",
    "        self.class_n = class_n\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.mode = mode\n",
    "        # 모델\n",
    "        if self.model_name == 'resnet50':\n",
    "            self.encoder = Resnet50(class_n=class_n)\n",
    "        elif self.model_name == 'convnext_large_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True)\n",
    "        elif self.model_name == 'convnext_base_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True)\n",
    "        else:\n",
    "            if self.drop_path_rate != 0 :\n",
    "                if self.mode == 'train' :\n",
    "                    self.encoder = timm.create_model(self.model_name, pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "                else:\n",
    "                    self.encoder = timm.create_model(model_name, pretrained=True)\n",
    "            else:\n",
    "                self.encoder = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "        names = []\n",
    "        modules = []\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            names.append(name)\n",
    "            modules.append(module)\n",
    "\n",
    "        self.fc_in_features = self.encoder.num_features\n",
    "        print(f'The layer was modified...')\n",
    "\n",
    "        fc_name = names[-1].split('.')\n",
    "\n",
    "        if len(fc_name)==1:\n",
    "            print(f'{getattr(self.encoder,fc_name[0])} -> Linear(in_features={self.fc_in_features}, out_features={class_n}, bias=True)')\n",
    "            setattr(self.encoder, fc_name[0], nn.Linear(self.fc_in_features, class_n))\n",
    "        elif len(fc_name)==2:\n",
    "            print(f'{getattr(getattr(self.encoder,fc_name[0]),fc_name[1])} -> Linear(in_features={self.fc_in_features}, out_features={class_n}, bias=True)')\n",
    "            setattr(getattr(self.encoder,fc_name[0]), fc_name[1], nn.Linear(self.fc_in_features, class_n))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class LSTM_Decoder(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim, num_features, cnn_features_len, class_n, rate):\n",
    "        super(LSTM_Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(max_len, embedding_dim)\n",
    "        # self.lstm_fc = nn.Linear(embedding_dim * 2 if bidirectional else embedding_dim, 2048)\n",
    "        self.lstm_fc = nn.Linear(num_features*embedding_dim, 2048)\n",
    "        self.final_layer = nn.Linear(cnn_features_len + 2048, class_n)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, enc_out, dec_inp):\n",
    "        self.lstm.flatten_parameters()\n",
    "        hidden, _ = self.lstm(dec_inp)\n",
    "        hidden = hidden.view(hidden.size(0), -1)\n",
    "        hidden = self.lstm_fc(hidden)\n",
    "        concat = torch.cat([enc_out, hidden], dim=1) # enc_out + hidden\n",
    "        fc_input = concat\n",
    "        output = self.dropout((self.final_layer(fc_input)))\n",
    "        return output\n",
    "\n",
    "\n",
    "class Resnet50(nn.Module):\n",
    "    def __init__(self, class_n):\n",
    "        super(Resnet50, self).__init__()\n",
    "        self.base = nn.Sequential(OrderedDict(list(models.resnet50(pretrained=True).named_children())[:-2]))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(2048, class_n)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, features):\n",
    "        cosine = F.linear(F.normalize(features), F.normalize(self.weight))\n",
    "        return cosine\n",
    "\n",
    "\n",
    "class ArcfaceImageModel(nn.Module):\n",
    "    def __init__(self, model_name, class_n, drop_path_rate, embedding_dim=1024, mode='train', encode=False):\n",
    "        super().__init__()\n",
    "        self.model_name = '_'.join(model_name.lower().split('_')[1:])\n",
    "        self.class_n = class_n\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mode = mode\n",
    "        self.encode = encode\n",
    "        # 모델\n",
    "        if self.model_name == 'resnet50':\n",
    "            self.encoder = Resnet50(class_n=class_n)\n",
    "        elif self.model_name == 'convnext_xlarge_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_xlarge_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_xlarge_384_in22ft1k(pretrained=True)\n",
    "        elif self.model_name == 'convnext_large_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_large_384_in22ft1k(pretrained=True)\n",
    "        elif self.model_name == 'convnext_base_384_in22ft1k':\n",
    "            if self.mode == 'train' :\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "            else:\n",
    "                self.encoder = timm.models.convnext_base_384_in22ft1k(pretrained=True)\n",
    "        else:\n",
    "            if self.drop_path_rate != 0 :\n",
    "                if self.mode == 'train' :\n",
    "                    self.encoder = timm.create_model(self.model_name, pretrained=True, drop_path_rate=self.drop_path_rate)\n",
    "                else:\n",
    "                    self.encoder = timm.create_model(self.model_name, pretrained=True)\n",
    "            else:\n",
    "                self.encoder = timm.create_model(self.model_name, pretrained=True)\n",
    "\n",
    "        names = []\n",
    "        modules = []\n",
    "        for name, module in self.encoder.named_modules():\n",
    "            names.append(name)\n",
    "            modules.append(module)\n",
    "\n",
    "        self.fc_in_features = self.encoder.num_features\n",
    "        print(f'The layer was modified...')\n",
    "\n",
    "        fc_name = names[-1].split('.')\n",
    "\n",
    "        if len(fc_name)==1:\n",
    "            print(f'{getattr(self.encoder,fc_name[0])} -> Linear(in_features={self.fc_in_features}, out_features={1000}, bias=True)')\n",
    "            setattr(self.encoder, fc_name[0], nn.Linear(self.fc_in_features, 1000))\n",
    "        elif len(fc_name)==2:\n",
    "            print(f'{getattr(getattr(self.encoder,fc_name[0]),fc_name[1])} -> Linear(in_features={self.fc_in_features}, out_features={1000}, bias=True)')\n",
    "            setattr(getattr(self.encoder,fc_name[0]), fc_name[1], nn.Linear(self.fc_in_features, 1000))\n",
    "\n",
    "        self.neck = nn.Sequential(\n",
    "            nn.Linear(1000, self.embedding_dim, bias=True),\n",
    "            nn.BatchNorm1d(self.embedding_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.arc_margin_product = ArcMarginProduct(self.embedding_dim, self.class_n)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.neck(x)\n",
    "        logits = self.arc_margin_product(x)\n",
    "        if self.encode :\n",
    "            return x\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "class ImageModel2LSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name,\n",
    "            pretrained_model_path,\n",
    "            max_len,\n",
    "            img_embedding_dim,\n",
    "            env_embedding_dim,\n",
    "            num_features,\n",
    "            class_n,\n",
    "            dropout_rate=0.1,\n",
    "            mode='train'\n",
    "    ):\n",
    "        super(ImageModel2LSTMModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.pretrained_model_path = pretrained_model_path\n",
    "        self.mode = mode\n",
    "        self.class_n = class_n\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_len = max_len\n",
    "        self.img_embedding_dim = img_embedding_dim\n",
    "        self.env_embedding_dim = env_embedding_dim\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # When using new data (existing data + aihub pepper white powder data), 25 when it's not 28\n",
    "        if self.pretrained_model_path :\n",
    "            self.encoder = ArcfaceImageModel(model_name, 25, drop_path_rate=0, embedding_dim=self.img_embedding_dim,\n",
    "                                             mode='test', encode=True)\n",
    "            self.encoder.load_state_dict(torch.load(self.pretrained_model_path)['model_state_dict'])\n",
    "            self.encoder.requires_grad = False\n",
    "        else:\n",
    "            self.encoder = ArcfaceImageModel(model_name, 25, drop_path_rate=0.2, embedding_dim=self.img_embedding_dim,\n",
    "                                             mode='train', encode=True)\n",
    "\n",
    "        self.rnn = LSTM_Decoder(self.max_len, self.env_embedding_dim, self.num_features, cnn_features_len=self.img_embedding_dim, class_n=self.class_n, rate=self.dropout_rate)\n",
    "\n",
    "    def forward(self, img, seq):\n",
    "        cnn_output = self.encoder(img)\n",
    "        output = self.rnn(cnn_output, seq)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class FocalLoss(nn.modules.loss._WeightedLoss):\n",
    "    def __init__(self, weight=None, gamma=2, factor=0.1, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__(weight, reduction=reduction)\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # weight parameter will act as the alpha parameter to balance class weights\n",
    "        self.factor = factor\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        # num_classes = input.shape[-1]\n",
    "        # target = smooth(target.float(), num_classes, self.factor)\n",
    "        ce_loss = F.cross_entropy(input, target, reduction=self.reduction, weight=self.weight)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss).mean()\n",
    "        return focal_loss\n",
    "\n",
    "\"\"\"\n",
    "https://github.com/dongkyuk/DOLG-pytorch/blob/main/model/arcface.py\n",
    "\"\"\"\n",
    "class ArcFaceLoss(nn.Module):\n",
    "    def __init__(self, scale_factor=45.0, margin=0.10, criterion=None, weight=None):\n",
    "        super(ArcFaceLoss, self).__init__()\n",
    "\n",
    "        if criterion:\n",
    "            self.criterion = criterion\n",
    "        else:\n",
    "            if weight:\n",
    "                self.criterion = nn.CrossEntropyLoss(weight=weight)\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss()\n",
    "        self.margin = margin\n",
    "        self.scale_factor = scale_factor\n",
    "\n",
    "\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, logits, label):\n",
    "        # input is not l2 normalized\n",
    "        logits = logits.float()\n",
    "        cosine = logits\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = phi.type(cosine.type())\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "\n",
    "        one_hot = torch.zeros(cosine.size(), device=logits.device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "\n",
    "        logit = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        logit *= self.scale_factor\n",
    "\n",
    "        loss = self.criterion(logit, label)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def rand_bbox(W, H, lam):\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(cut_w // 2, W - cut_w // 2)\n",
    "    cy = np.random.randint(cut_h // 2, H - cut_h // 2)\n",
    "\n",
    "    bbx1 = cx - cut_w // 2\n",
    "    bby1 = cy - cut_h // 2\n",
    "    bbx2 = cx + cut_w // 2\n",
    "    bby2 = cy + cut_h // 2\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "def cutmix(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(x.size()[2], x.size()[3], lam)\n",
    "    x[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "class WarmUpLR(_LRScheduler):\n",
    "    \"\"\"warmup_training learning rate scheduler\n",
    "    Args:\n",
    "        optimizer: optimzier(e.g. SGD)\n",
    "        total_iters: totoal_iters of warmup phase\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        self.total_iters = total_iters\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"we will use the first m batches, and set the learning\n",
    "        rate to base_lr * m / total_iters\n",
    "        \"\"\"\n",
    "        return [base_lr * self.last_epoch / (self.total_iters + 1e-8) for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_logger(save_dir, comment=None):\n",
    "    c_date, c_time = datetime.now().strftime(\"%Y%m%d/%H%M%S\").split('/')\n",
    "    if comment is not None:\n",
    "        if os.path.exists(os.path.join(save_dir, c_date, comment)):\n",
    "            comment += f'_{c_time}'\n",
    "    else:\n",
    "        comment = c_time\n",
    "    log_dir = os.path.join(save_dir, c_date, comment)\n",
    "    log_txt = os.path.join(log_dir, 'log.txt')\n",
    "\n",
    "    os.makedirs(f'{log_dir}/ckpts')\n",
    "    os.makedirs(f'{log_dir}/submissions')\n",
    "    os.makedirs(f'{log_dir}/comparisons')\n",
    "\n",
    "    global logger\n",
    "    logger = logging.getLogger(c_time)\n",
    "\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger = logging.getLogger(c_time)\n",
    "\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(message)s\", datefmt='%Y-%m-%d %H:%M:%S')\n",
    "    h_file = logging.FileHandler(filename=log_txt, mode='a')\n",
    "    h_file.setFormatter(fmt)\n",
    "    h_file.setLevel(logging.INFO)\n",
    "    logger.addHandler(h_file)\n",
    "    logger.info(f'Log directory ... {log_txt}')\n",
    "    return log_dir\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    real = real.cpu()\n",
    "    pred = torch.argmax(pred, dim=1).cpu()\n",
    "    acc = (pred == real).sum()/real.shape[0]\n",
    "    score = f1_score(real, pred, average='macro')\n",
    "    return acc, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, warmup_scheduler, scheduler, scaler, epoch, wandb, args):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc = 0\n",
    "    total_train_score = 0\n",
    "    batch_iter = tqdm(enumerate(train_loader), 'Training', total=len(train_loader), ncols=150)\n",
    "\n",
    "    # breakpoint()\n",
    "    lam = None\n",
    "    label_a, label_b = None, None\n",
    "    for batch_idx, batch_item in batch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        label = batch_item['label'].cuda()\n",
    "\n",
    "        if epoch <= args.warm_epoch:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "        if args.cutmix:\n",
    "            r = np.random.rand(1)\n",
    "            if r < args.cutmix_prob:\n",
    "                img, label_a, label_b, lam = cutmix(img, label)\n",
    "\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    if args.environment_feature:\n",
    "                        csv_feature = batch_item['csv_feature'].cuda()\n",
    "                        output = model(img, csv_feature)\n",
    "                    else:\n",
    "                        output = model(img)\n",
    "            else:\n",
    "                if args.environment_feature:\n",
    "                    csv_feature = batch_item['csv_feature'].cuda()\n",
    "                    output = model(img, csv_feature)\n",
    "                else:\n",
    "                    output = model(img)\n",
    "            if r < args.cutmix_prob:\n",
    "                train_loss = lam * criterion(output, label_a) + (1 - lam) * criterion(output, label_b)\n",
    "            else:\n",
    "                train_loss = criterion(output, label)\n",
    "        else:\n",
    "            if args.amp:\n",
    "                with autocast():\n",
    "                    if args.environment_feature:\n",
    "                        csv_feature = batch_item['csv_feature'].cuda()\n",
    "                        output = model(img, csv_feature)\n",
    "                    else:\n",
    "                        output = model(img)\n",
    "            else:\n",
    "                if args.environment_feature:\n",
    "                    csv_feature = batch_item['csv_feature'].cuda()\n",
    "                    output = model(img, csv_feature)\n",
    "                else:\n",
    "                    output = model(img)\n",
    "\n",
    "            train_loss = criterion(output, label)\n",
    "\n",
    "        if args.amp:\n",
    "            scaler.scale(train_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if args.scheduler == 'cycle':\n",
    "            if epoch > args.warm_epoch:\n",
    "                scheduler.step()\n",
    "\n",
    "        train_acc, train_score = accuracy_function(label, output)\n",
    "        total_train_loss += train_loss\n",
    "        total_train_acc += train_acc\n",
    "        total_train_score += train_score\n",
    "        log = f'[EPOCH {epoch}] Train Loss : {train_loss.item():.4f}({total_train_loss / (batch_idx + 1):.4f}), '\n",
    "        log += f'Train Acc : {train_acc.item():.4f}({total_train_acc / (batch_idx + 1):.4f}), '\n",
    "        log += f'Train F1 : {train_score.item():.4f}({total_train_score / (batch_idx + 1):.4f})'\n",
    "        if batch_idx+1 == len(batch_iter):\n",
    "            log = f'[EPOCH {epoch}] Train Loss : {total_train_loss / (batch_idx + 1):.4f}, '\n",
    "            log += f'Train Acc : {total_train_acc / (batch_idx + 1):.4f}, '\n",
    "            log += f'Train F1 : {total_train_score / (batch_idx + 1):.4f}, '\n",
    "            log += f\"LR : {optimizer.param_groups[0]['lr']:.2e}\"\n",
    "\n",
    "\n",
    "        batch_iter.set_description(log)\n",
    "        batch_iter.update()\n",
    "\n",
    "\n",
    "\n",
    "    _lr = optimizer.param_groups[0]['lr']\n",
    "    train_mean_loss = total_train_loss / len(batch_iter)\n",
    "    train_mean_acc = total_train_acc / len(batch_iter)\n",
    "    train_mean_f1 = total_train_score / len(batch_iter)\n",
    "\n",
    "    logger.info(log)\n",
    "    batch_iter.close()\n",
    "\n",
    "    if args.wandb:\n",
    "        wandb.log({'train_mean_loss': train_mean_loss, 'lr': _lr, 'train_mean_acc': train_mean_acc, 'train_mean_f1': train_mean_f1}, step=epoch)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid(model, val_loader, criterion, epoch, wandb, args):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_acc = 0\n",
    "    total_val_score = 0\n",
    "\n",
    "    batch_iter = tqdm(enumerate(val_loader), 'Validating', total=len(val_loader), ncols=150)\n",
    "\n",
    "    for batch_idx, batch_item in batch_iter:\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        label = batch_item['label'].cuda()\n",
    "\n",
    "        if args.environment_feature:\n",
    "            csv_feature = batch_item['csv_feature'].cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(img)\n",
    "\n",
    "        val_loss = criterion(output, label)\n",
    "        val_acc, val_score = accuracy_function(label, output)\n",
    "        total_val_loss += val_loss\n",
    "        total_val_acc += val_acc\n",
    "        total_val_score += val_score\n",
    "\n",
    "        log = f'[EPOCH {epoch}] Valid Loss : {val_loss.item():.4f}({total_val_loss / (batch_idx + 1):.4f}), '\n",
    "        log += f'Valid Acc : {val_acc.item():.4f}({total_val_acc / (batch_idx + 1):.4f}), '\n",
    "        log += f'Valid F1 : {val_score.item():.4f}({total_val_score / (batch_idx + 1):.4f})'\n",
    "        if batch_idx+1 == len(batch_iter):\n",
    "            log = f'[EPOCH {epoch}] Valid Loss : {total_val_loss / (batch_idx + 1):.4f}, '\n",
    "            log += f'Valid Acc : {total_val_acc / (batch_idx + 1):.4f}, '\n",
    "            log += f'Valid F1 : {total_val_score / (batch_idx + 1):.4f}'\n",
    "        batch_iter.set_description(log)\n",
    "        batch_iter.update()\n",
    "\n",
    "    val_mean_loss = total_val_loss / len(batch_iter)\n",
    "    val_mean_acc = total_val_acc / len(batch_iter)\n",
    "    val_mean_f1 = total_val_score / len(batch_iter)\n",
    "    logger.info(log)\n",
    "    batch_iter.set_description(log)\n",
    "    batch_iter.close()\n",
    "\n",
    "    if args.wandb:\n",
    "        wandb.log({'valid_mean_loss': val_mean_loss,'valid_mean_acc': val_mean_acc, 'valid_mean_f1': val_mean_f1}, step=epoch)\n",
    "\n",
    "    return val_mean_loss, val_mean_acc, val_mean_f1\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, test_loader, label_decoder, epoch, fold, wandb, args):\n",
    "    model.eval()\n",
    "    batch_iter = tqdm(enumerate(test_loader), 'Testing', total=len(test_loader),\n",
    "                      leave=False)\n",
    "    preds = []\n",
    "    outputs = None\n",
    "    output_path = f'{args.log_dir}/submissions/output_ep{epoch:03d}_fold{fold:02d}_{args.model}.pt'\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (batch, batch_item) in enumerate(batch_iter):\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        if args.environment_feature:\n",
    "            csv_feature = batch_item['csv_feature'].cuda()\n",
    "            with torch.no_grad():\n",
    "                output = model(img, csv_feature)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(img)\n",
    "        if i == 0:\n",
    "            outputs = output.cpu()\n",
    "        else:\n",
    "            outputs = torch.cat((outputs, output.cpu()), 0)\n",
    "\n",
    "        output = torch.argmax(output, dim=1).clone().cpu().numpy()\n",
    "        preds.extend(output)\n",
    "\n",
    "    preds = np.array([label_decoder[int(val)] for val in preds])\n",
    "    submission = pd.read_csv(f'{args.data_path}/sample_submission.csv')\n",
    "    submission['label'] = preds\n",
    "    submission.to_csv(f'{args.log_dir}/submissions/submission_ep{epoch:03d}_fold{fold:02d}_{args.model}.csv', index=False)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    torch.save(outputs, output_path)\n",
    "    del outputs\n",
    "    if args.wandb:\n",
    "        wandb.log({'preprocess-infer-save-time.': end - start}, step=epoch)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_label(model, loader, label_description, label_decoder, epoch, fold, args):\n",
    "    model.eval()\n",
    "    batch_iter = tqdm(enumerate(loader), 'Predicting', total=len(loader),\n",
    "                      leave=False)\n",
    "    preds = []\n",
    "    answer = []\n",
    "\n",
    "    for batch, batch_item in batch_iter:\n",
    "        img = batch_item['img']['image'].cuda()\n",
    "        if args.environment_feature:\n",
    "            csv_feature = batch_item['csv_feature'].cuda()\n",
    "            output = model(img, csv_feature)\n",
    "        else:\n",
    "            output = model(img)\n",
    "\n",
    "        output = torch.argmax(output, dim=1).clone().cpu().numpy()\n",
    "        preds.extend(output)\n",
    "        answer.extend(batch_item['label'])\n",
    "\n",
    "    answer = np.array([label_description[label_decoder[int(val)]] for val in answer])\n",
    "    preds = np.array([label_description[label_decoder[int(val)]] for val in preds])\n",
    "    new_crosstab = pd.crosstab(answer, preds, rownames=['answer'], colnames=['preds'])\n",
    "    new_crosstab.to_csv(f'{args.log_dir}/comparisons/comparison_ep{epoch:03d}_fold{fold:02d}_{args.model}.csv', index=True)\n",
    "    # print(new_crosstab)\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ensemble_5fold_pt(model_name, output_path_list, label_decoder, args):\n",
    "    predict_list = []\n",
    "\n",
    "    for output_path in output_path_list:\n",
    "        outputs = torch.load(output_path)\n",
    "        preds = torch.softmax(outputs, dim=1).clone().detach().cpu().numpy()\n",
    "        predict_list.append(np.array(preds))\n",
    "\n",
    "    # ensemble = np.array(predict_list[0] + predict_list[1] + predict_list[2]) / len(predict_list)\n",
    "    ensemble = np.array(predict_list[0] + predict_list[1] + predict_list[2] + predict_list[3] + predict_list[4])/len(predict_list)\n",
    "\n",
    "    ensemble = np.argmax(ensemble,axis=1)\n",
    "    ensemble = np.array([label_decoder[val] for val in ensemble])\n",
    "    submission = pd.read_csv(f'{args.data_path}/sample_submission.csv')\n",
    "    submission['label'] = ensemble\n",
    "    csv_name = f'submission_avg_{args.data_split.lower()}_{\"_\".join(model_name.split(\"_\"))}_ep{args.epochs:03d}_'\n",
    "    csv_name +=f'{args.image_size}_{args.optimizer}'\n",
    "    csv_name +=f'_aug_{args.aug_ver:03d}_loss_{args.loss}_amp_{args.amp}_csv_{args.environment_feature}.csv'\n",
    "    submission.to_csv(csv_name, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('-dp', '--data_path', type=str, default='./data',\n",
    "#                         help='Data root path.')\n",
    "#     parser.add_argument('-sp', '--save_path', type=str, default='/mldisk/nfs_shared_/dh/weights/plant-dacon',\n",
    "#                         help='Directory where the generated files will be stored')\n",
    "#     parser.add_argument('-c', '--comment', type=str, default=None)\n",
    "\n",
    "#     parser.add_argument('-e', '--epochs', type=int, default=20,\n",
    "#                         help='Number of epochs to train the DML network. Default: 30')\n",
    "#     parser.add_argument('-we', '--warm_epoch', type=int, default=0,\n",
    "#                         help='Number of warmup epochs to train the DML network. Default: 0')\n",
    "\n",
    "#     parser.add_argument('-bs', '--batch_size', type=int, default=28,\n",
    "#                         help='The size of the image you want to preprocess. '\n",
    "#                              'Default: 32')\n",
    "#     parser.add_argument('-is', '--image_size', type=int, default=384,\n",
    "#                         help='Variables to resize the image. '\n",
    "#                              'Default: 384')\n",
    "#     parser.add_argument('-nw', '--num_workers', type=int, default=16,\n",
    "#                         help='Number of workers of dataloader')\n",
    "\n",
    "#     # augmentation configs:\n",
    "#     parser.add_argument('-av', '--aug_ver', type=int, default=29,\n",
    "#                         help='Name of Data Augmentation(Refer to dataset.py). Options: \"0 (No Aug) , 1 ~ 20.')\n",
    "#     parser.add_argument('-cm', '--cutmix', type=bool, default=False,\n",
    "#                         help='Cutmix Auemtnation. Default: True.')\n",
    "#     parser.add_argument('-cmp', '--cutmix_prob', type=float, default=0.25,\n",
    "#                         help='Cutmix Auemtnation Probability. Default: 0.25.')\n",
    "\n",
    "\n",
    "#     # loss configs:\n",
    "#     parser.add_argument('-l', '--loss', type=str, default='focal',\n",
    "#                         help='Name of loss function. Options: \"ce, focal, arcface')\n",
    "\n",
    "#     parser.add_argument('-cw', '--class_weights', type=bool, default=False)\n",
    "\n",
    "#     # optimizer configs:\n",
    "#     parser.add_argument('-ot', '--optimizer', type=str, default='adamw',\n",
    "#                         help='Name of Optimizer. Options: \"adam, radam, adamw, adamp, ranger, lamb')\n",
    "#     parser.add_argument('-sc', '--scheduler', type=str, default='cos_base',\n",
    "#                         help='Name of Optimizer. Options: \"cos_base, cos, cos_warm_restart, cycle')\n",
    "#     # Adam\n",
    "#     parser.add_argument('-lr', '--learning_rate', type=float, default=1e-4,\n",
    "#                         help='Learning rate of the DML network. Default: 10^-4')\n",
    "#     parser.add_argument('-wd', '--weight_decay', type=float, default=1e-3,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "\n",
    "#     # Step\n",
    "#     parser.add_argument('-st', '--step_size', type=int, default=3)\n",
    "#     parser.add_argument('-sg', '--step_gamma', type=float, default=0.8)\n",
    "\n",
    "#     # Cycle\n",
    "#     parser.add_argument('-mal', '--max_lr', type=float, default=1e-3,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "\n",
    "#     # Cosine Annealing\n",
    "#     parser.add_argument('-tm', '--tmax', type=int, default=60,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "#     parser.add_argument('-mil', '--min_lr', type=float, default=1e-6,\n",
    "#                         help='Regularization parameter of the DML network. Default: 10^-4')\n",
    "\n",
    "#     # data split configs:\n",
    "#     parser.add_argument('-ds', '--data_split', type=str, default='StratifiedKFold',\n",
    "#                         help='Name of Training Data Sampling Strategy. Options: \"Split_base, Stratified, StratifiedKFold, KFold')\n",
    "#     parser.add_argument('-ns', '--n_splits', type=int, default=5,\n",
    "#                         help='The number of datasets(Train,val) to be divided.')\n",
    "#     parser.add_argument('-rs', '--random_seed', type=int, default=42,\n",
    "#                         help='Random Seed')\n",
    "#     parser.add_argument('-vr', '--val_ratio', type=float, default=0.2,\n",
    "#                         help='validation dataset ratio')\n",
    "\n",
    "\n",
    "#     # image model specific configs:\n",
    "#     parser.add_argument('-m', '--model', type=str, default='arc_convnext_xlarge_384_in22ft1k',\n",
    "#                         help='Name of model. Options: Refer to image_model_list.txt. '\n",
    "#                              'Option : \"convnext_large_384_in22ft1k, swin_large_patch4_window12_384_in22k'\n",
    "#                              'arc_convnext_large_384_in22ft1k, arc_swin_large_patch4_window12_384_in22k'\n",
    "#                              'If you want to use arcface loss while learning only the image model, '\n",
    "#                              'set the \"arc_xxx\" model and the \"arcface\" loss function and environment feature to \"FALSE\".'\n",
    "#                              'If you want to use focal loss while learning the multi-modal model, ' \n",
    "#                              'set the \"arc_xxx\" model and the \"focal\" loss function and environment feature to \"True\".')\n",
    "\n",
    "#     parser.add_argument('-dpr', '--drop_path_rate', type=float, default=0.2,\n",
    "#                         help='dropout rate')\n",
    "\n",
    "#     # multi-modal model specific configs:\n",
    "#     parser.add_argument('-ied', '--img_embedding_dim', type=int, default=1024,\n",
    "#                         help='Dimension of the output embedding of image model')\n",
    "#     parser.add_argument('-eed', '--env_embedding_dim', type=int, default=1024,\n",
    "#                         help='Dimension of the of environment feature model')\n",
    "\n",
    "#     parser.add_argument('-ml', '--max_len', type=int, default=320,\n",
    "#                         help='Number of layers')\n",
    "#     parser.add_argument('-dr', '--dropout_rate', type=float, default=0.2,\n",
    "#                         help='dropout rate')\n",
    "\n",
    "#     # feature configs:\n",
    "#     parser.add_argument('-ef', '--environment_feature', type=bool, default=True,\n",
    "#                         help='Whether to use environmental features'\n",
    "#                         )\n",
    "\n",
    "#     # eval configs:\n",
    "#     parser.add_argument('-d', '--dataset', type=str, default='PlantDACON',\n",
    "#                         help='Name of evaluation dataset. Options: \"PlantDACON, PlantVillage')\n",
    "\n",
    "#     # wandb config:\n",
    "#     parser.add_argument('--wandb', type=bool, default=False,\n",
    "#                         help='wandb'\n",
    "#                         )\n",
    "\n",
    "#     # amp config:\n",
    "#     parser.add_argument('--amp', type=bool, default=True,\n",
    "#                         help='amp mode'\n",
    "#                         )\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    import easydict\n",
    " \n",
    "    args = easydict.EasyDict({\n",
    "\n",
    "            \"data_path\": './data',\n",
    "\n",
    "            \"save_path\": './weights/plant-lg-dacon',\n",
    "\n",
    "            \"comment\": None,\n",
    "            \n",
    "            \"epochs\" : 20,\n",
    "            \n",
    "            \"warm_epoch\": 0,\n",
    "\n",
    "            \"batch_size\": 28,\n",
    "\n",
    "            \"image_size\": 384,\n",
    "            \n",
    "            \"num_workers\": 16,\n",
    "            \n",
    "            # augmentation configs:\n",
    "            \"aug_ver\": 29,\n",
    "\n",
    "            \"cutmix\": False,\n",
    "\n",
    "            \"cutmix_prob\": 0.25,\n",
    "            \n",
    "            # loss configs:\n",
    "            \"loss\": 'focal',\n",
    "\n",
    "            \"class_weights\": False,\n",
    "            \n",
    "            # optimizer configs:\n",
    "            \"optimizer\": 'adamw',\n",
    "\n",
    "            \"scheduler\": 'cos_base',\n",
    "\n",
    "            # Adam \n",
    "            \"learning_rate\": 1e-4,\n",
    "\n",
    "            \"weight_decay\": 1e-3,\n",
    "            \n",
    "            # Step\n",
    "            \"step_size\": 3,\n",
    "            \n",
    "            \"step_gamma\": 0.8,\n",
    "            \n",
    "            # Cycle\n",
    "            \"max_lr\": 1e-3,\n",
    "            \n",
    "            # Cosine Annealing\n",
    "            \"tmax\": 60,\n",
    "\n",
    "            \"min_lr\": 1e-6,\n",
    "            \n",
    "            # data split configs:\n",
    "            \"data_split\": 'StratifiedKFold',\n",
    "\n",
    "            \"n_splits\": 5,\n",
    "            \n",
    "            \"random_seed\": 42,\n",
    "            \n",
    "            \"val_ratio\" : 0.2,\n",
    "            \n",
    "            # image model specific configs:\n",
    "            \"model\": \"arc_convnext_xlarge_384_in22ft1k\",\n",
    "\n",
    "            \"drop_path_rate\": 0.2,\n",
    "            \n",
    "            # multi-modal model specific configs:\n",
    "            \"img_embedding_dim\": 1024,\n",
    "            \n",
    "            \"env_embedding_dim\": 1024,\n",
    "            \n",
    "            \"max_len\" : 320,\n",
    "            \n",
    "            \"dropout_rate\" :0.2,\n",
    "            \n",
    "            # feature configs:\n",
    "            \"environment_feature\": True,\n",
    "            \n",
    "            # eval configs:\n",
    "            \"dataset\": 'PlantDACON',\n",
    "            \n",
    "            # wandb config:\n",
    "            \"wandb\": False,\n",
    "            \n",
    "            # amp config:\n",
    "            \"amp\": True,\n",
    "\n",
    "            \n",
    "\n",
    "    })\n",
    "\n",
    "\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "    # Data Path\n",
    "    train_data = sorted(glob(f'{args.data_path}/train/*'))\n",
    "    csv_files = sorted(glob(f'{args.data_path}/train/*/*.csv'))\n",
    "    test_data = sorted(glob(f'{args.data_path}/test/*'))\n",
    "    labelsss = pd.read_csv(f'{args.data_path}/train.csv')['label']\n",
    "\n",
    "    folds = []\n",
    "    # Data Split\n",
    "    if args.data_split.lower() == 'split_base':\n",
    "        train_data, val_data = train_test_split(train_data, random_state=args.random_seed, test_size=args.val_ratio, shuffle=True)\n",
    "        folds.append((train_data, val_data))\n",
    "        args.n_split = 1\n",
    "    elif args.data_split.lower() == 'stratified':\n",
    "        train_data, val_data = train_test_split(train_data, random_state=args.random_seed, test_size=args.val_ratio, stratify=labelsss, shuffle=True)\n",
    "        folds.append((train_data, val_data))\n",
    "        args.n_split = 1\n",
    "    elif args.data_split.lower() == 'stratifiedkfold':\n",
    "        train_data = np.array(train_data)\n",
    "        skf = StratifiedKFold(n_splits=args.n_splits, random_state=args.random_seed, shuffle=True)\n",
    "        for train_idx, valid_idx in skf.split(train_data,labelsss.tolist()):\n",
    "            folds.append((train_data[train_idx].tolist(), train_data[valid_idx].tolist()))\n",
    "\n",
    "    elif args.data_split == 'kfold':\n",
    "        train_data = np.array(train_data)\n",
    "        kf = KFold(n_splits=args.n_splits, random_state=args.random_seed, shuffle=True)\n",
    "        for train_idx, valid_idx in kf.split(train_data,labelsss.tolist()):\n",
    "            folds.append((train_data[train_idx].tolist(), train_data[valid_idx].tolist()))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Available CSV feature for Training and Testing\n",
    "    if args.environment_feature:\n",
    "        avail_features = ['내부 온도 1 평균', '내부 온도 1 최고', '내부 온도 1 최저', '내부 습도 1 평균', '내부 습도 1 최고',\n",
    "                        '내부 습도 1 최저', '내부 이슬점 평균', '내부 이슬점 최고', '내부 이슬점 최저']\n",
    "    else:\n",
    "        avail_features = None\n",
    "\n",
    "    # Label Description : Refer to CSV File\n",
    "    label_description = {\n",
    "        \"1_00_0\": \"딸기\",\n",
    "        \"2_00_0\": \"토마토\",\n",
    "        \"2_a5_2\": \"토마토_흰가루병_중기\",\n",
    "        \"3_00_0\": \"파프리카\",\n",
    "        \"3_a9_1\": \"파프리카_흰가루병_초기\",\n",
    "        \"3_a9_2\": \"파프리카_흰가루병_중기\",\n",
    "        \"3_a9_3\": \"파프리카_흰가루병_말기\",\n",
    "        \"3_b3_1\": \"파프리카_칼슘결핍_초기\",\n",
    "        \"3_b6_1\": \"파프리카_다량원소결필(N)_초기\",\n",
    "        \"3_b7_1\": \"파프리카_다량원소결필(P)_초기\",\n",
    "        \"3_b8_1\": \"파프리카_다량원소결필(K)_초기\",\n",
    "        \"4_00_0\": \"오이\",\n",
    "        \"5_00_0\": \"고추\",\n",
    "        \"5_a7_2\": \"고추_탄저병_중기\",\n",
    "        \"5_b6_1\": \"고추_다량원소결필(N)_초기\",\n",
    "        \"5_b7_1\": \"고추_다량원소결필(P)_초기\",\n",
    "        \"5_b8_1\": \"고추_다량원소결필(K)_초기\",\n",
    "        \"6_00_0\": \"시설포도\",\n",
    "        \"6_a11_1\": \"시설포도_탄저병_초기\",\n",
    "        \"6_a11_2\": \"시설포도_탄저병_중기\",\n",
    "        \"6_a12_1\": \"시설포도_노균병_초기\",\n",
    "        \"6_a12_2\": \"시설포도_노균병_중기\",\n",
    "        \"6_b4_1\": \"시설포도_일소피해_초기\",\n",
    "        \"6_b4_3\": \"시설포도_일소피해_말기\",\n",
    "        \"6_b5_1\": \"시설포도_축과병_초기\"\n",
    "    }\n",
    "\n",
    "\n",
    "    label_encoder = {key: idx for idx, key in enumerate(label_description)}\n",
    "    label_decoder = {val: key for key, val in label_encoder.items()}\n",
    "\n",
    "    if args.class_weights :\n",
    "        val_counts = labelsss.value_counts().sort_index().values\n",
    "        class_weights = 1/np.log1p(val_counts)\n",
    "        class_weights = (class_weights / class_weights.sum()) * len(label_encoder.keys())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "\n",
    "    # Criterion\n",
    "    if args.loss == 'ce':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif args.loss == 'focal':\n",
    "        criterion = FocalLoss()\n",
    "    elif args.loss == 'arcface':\n",
    "        criterion = ArcFaceLoss(criterion=FocalLoss(), weight=class_weights)\n",
    "        # criterion = ArcFaceLoss(criterion=FocalLoss())\n",
    "\n",
    "    print(f'The number of datasets separated : {len(folds)}')\n",
    "    best_model_paths = []\n",
    "    output_path_list = []\n",
    "    best_image_model_paths = [None]*len(folds)\n",
    "    \n",
    "    for fold in range(len(folds)):\n",
    "        train_data, val_data = folds[fold]\n",
    "\n",
    "        # Multi-modal Model\n",
    "        if args.environment_feature:\n",
    "            model = ImageModel2LSTMModel(model_name=args.model,\n",
    "                                         pretrained_model_path=best_image_model_paths[fold],\n",
    "                                         max_len=args.max_len,\n",
    "                                         img_embedding_dim=args.img_embedding_dim,\n",
    "                                         env_embedding_dim=args.env_embedding_dim,\n",
    "                                         num_features=len(avail_features),\n",
    "                                         class_n=len(label_encoder.keys()),\n",
    "                                         dropout_rate=0.1,\n",
    "                                         mode='train')\n",
    "\n",
    "        # Only-image Model\n",
    "        else:\n",
    "            if 'arc' in args.model:\n",
    "                model = ArcfaceImageModel(model_name=args.model,\n",
    "                                          class_n=len(label_encoder.keys()),\n",
    "                                          drop_path_rate=args.drop_path_rate,\n",
    "                                          mode='train')\n",
    "            else:\n",
    "                model = ImageModel(model_name=args.model,\n",
    "                                   class_n=len(label_encoder.keys()),\n",
    "                                   drop_path_rate=args.drop_path_rate,\n",
    "                                   mode='train')\n",
    "\n",
    "        model = nn.DataParallel(model.cuda())\n",
    "\n",
    "        # Dataset\n",
    "        train_dataset = globals()[args.dataset](args.image_size, train_data, csv_files, avail_features, label_description, args.aug_ver, mode='train')\n",
    "        val_dataset = globals()[args.dataset](args.image_size, val_data, csv_files, avail_features, label_description, mode='valid')\n",
    "\n",
    "\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size,\n",
    "                                                   num_workers=args.num_workers, shuffle=True)\n",
    "        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size,\n",
    "                                                 num_workers=args.num_workers, shuffle=False)\n",
    "\n",
    "\n",
    "        # Optimizer & Scheduler Setting\n",
    "        optimizer = None\n",
    "        if args.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                   lr=args.learning_rate,\n",
    "                                   weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'adamw':\n",
    "            optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=args.learning_rate,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'radam':\n",
    "            optimizer = optim.RAdam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                    lr=args.learning_rate,\n",
    "                                    weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'ranger':\n",
    "            optimizer = optim.Ranger(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                     lr=args.learning_rate,\n",
    "                                     betas=(0.9, 0.999),\n",
    "                                     weight_decay=args.weight_decay)\n",
    "        elif args.optimizer == 'lamb':\n",
    "            optimizer = optim.Lamb(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                                   lr=args.learning_rate,\n",
    "                                   weight_decay=args.weight_decay)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        iter_per_epoch = len(train_loader)\n",
    "        warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * args.warm_epoch)\n",
    "        scheduler = None\n",
    "        if args.scheduler == 'cos_base':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
    "        elif args.scheduler == 'cos_warm_restart':\n",
    "            args.epochs = 69\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, last_epoch=-1)\n",
    "        elif args.scheduler == 'cos':\n",
    "             # tmax = epoch * 2 => half-cycle\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.tmax, eta_min=args.min_lr)\n",
    "        elif args.scheduler == 'cycle':\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=args.max_lr, steps_per_epoch=iter_per_epoch, epochs=args.epochs)\n",
    "\n",
    "\n",
    "        # amp scaler\n",
    "        scaler = None\n",
    "        if args.amp:\n",
    "            scaler = GradScaler()\n",
    "\n",
    "        # log dir setting\n",
    "        log_dir = init_logger(args.save_path, args.comment)\n",
    "        args.log_dir = log_dir\n",
    "\n",
    "        # Wandb initialization\n",
    "        run = None\n",
    "        if args.wandb:\n",
    "            c_date, c_time = datetime.now().strftime(\"%m%d/%H%M%S\").split('/')\n",
    "            run = wandb.init(project=args.dataset, name=f'{args.model}_{c_date}_{c_time}_fold_{fold}')\n",
    "            wandb.config.update(args)\n",
    "\n",
    "        best_vacc, best_f1 = .0, .0\n",
    "        best_vloss = 9999.\n",
    "        best_epoch = 0\n",
    "\n",
    "        for epoch in range(1, args.epochs + 1):\n",
    "            train(model, train_loader, criterion, optimizer, warmup_scheduler, scheduler, scaler, epoch, wandb, args)\n",
    "            vloss, vacc, vf1 = valid(model, val_loader, criterion, epoch, wandb, args)\n",
    "            predict_label(model, val_loader, label_description, label_decoder, epoch, fold, args)\n",
    "            if vf1 > best_f1:\n",
    "                best_epoch = epoch\n",
    "                best_vloss = min(vloss, best_vloss)\n",
    "                best_f1 = max(vf1, best_f1)\n",
    "                if best_f1 > 0.9:\n",
    "                    torch.save({'model_state_dict': model.module.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler': scheduler.state_dict(),\n",
    "                                'epoch': epoch, },\n",
    "                               f'{log_dir}/ckpts/ckpt_epoch_{epoch:03d}_fold_{fold:01d}.pt')\n",
    "\n",
    "            if args.scheduler in ['cos_base', 'cos', 'cos_warm_restart']:\n",
    "                if epoch > args.warm_epoch:\n",
    "                    scheduler.step()\n",
    "\n",
    "        best_model_paths.append(f'{log_dir}/ckpts/ckpt_epoch_{best_epoch:03d}_fold_{fold:01d}.pt')\n",
    "\n",
    "        del model\n",
    "        del optimizer, scheduler\n",
    "        del train_dataset, val_dataset\n",
    "\n",
    "        ### Best Model Inference\n",
    "        # Multi-modal Model\n",
    "        if args.environment_feature:\n",
    "            model = ImageModel2LSTMModel(model_name=args.model,\n",
    "                                         pretrained_model_path=best_image_model_paths[fold],\n",
    "                                         max_len=args.max_len,\n",
    "                                         img_embedding_dim=args.img_embedding_dim,\n",
    "                                         env_embedding_dim=args.env_embedding_dim,\n",
    "                                         num_features=len(avail_features),\n",
    "                                         class_n=len(label_encoder.keys()),\n",
    "                                         dropout_rate=0,\n",
    "                                         mode='test')\n",
    "\n",
    "        # Only-image Model\n",
    "        else:\n",
    "            if 'arc' in args.model :\n",
    "                model = ArcfaceImageModel(model_name=args.model,\n",
    "                                          class_n=len(label_encoder.keys()),\n",
    "                                          drop_path_rate=0,\n",
    "                                          mode='test')\n",
    "            else:\n",
    "                model = ImageModel(model_name=args.model,\n",
    "                                   class_n=len(label_encoder.keys()),\n",
    "                                   drop_path_rate=0,\n",
    "                                   mode='test')\n",
    "        model.load_state_dict(torch.load(f'{log_dir}/ckpts/ckpt_epoch_{best_epoch:03d}_fold_{fold:01d}.pt')['model_state_dict'])\n",
    "        model = nn.DataParallel(model.cuda())\n",
    "\n",
    "        test_dataset = globals()[args.dataset](args.image_size, test_data, csv_files, avail_features,\n",
    "                                               label_description, mode='test')\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size,\n",
    "                                                  num_workers=args.num_workers, shuffle=False)\n",
    "        output_path = test(model, test_loader, label_decoder, best_epoch, fold, wandb, args)\n",
    "        output_path_list.append(output_path)\n",
    "        del test_dataset\n",
    "        del model\n",
    "\n",
    "        if args.wandb:\n",
    "            run.finish()\n",
    "\n",
    "    if args.data_split.lower() in ['stratifiedkfold','kfold'] :\n",
    "        ensemble_5fold_pt(model_name=args.model, output_path_list=output_path_list, label_decoder=label_decoder, args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of datasets separated : 5\n",
      "The layer was modified...\n",
      "Linear(in_features=2048, out_features=1000, bias=True) -> Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[EPOCH 1] Train Loss : 0.3894, Train Acc : 0.7635, Train F1 : 0.5464, LR : 1.00e-04: 100%|██████████████████████████| 165/165 [03:09<00:00,  1.15s/it]\n",
      "[EPOCH 1] Valid Loss : 0.0199, Valid Acc : 0.9011, Valid F1 : 0.7305: 100%|███████████████████████████████████████████| 42/42 [00:23<00:00,  1.76it/s]\n",
      "Training:   0%|                                                                                                               | 0/165 [00:00<?, ?it/s]             [W accumulate_grad.h:184] Warning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [256, 1, 7, 7], strides() = [49, 49, 7, 1]\n",
      "param.sizes() = [256, 1, 7, 7], strides() = [49, 1, 7, 1] (function operator())\n",
      "[EPOCH 2] Train Loss : 0.0514, Train Acc : 0.8819, Train F1 : 0.7172, LR : 9.94e-05: 100%|██████████████████████████| 165/165 [03:16<00:00,  1.19s/it]\n",
      "[EPOCH 2] Valid Loss : 0.0056, Valid Acc : 0.9501, Valid F1 : 0.8618: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "[EPOCH 3] Train Loss : 0.0371, Train Acc : 0.8984, Train F1 : 0.7577, LR : 9.76e-05: 100%|██████████████████████████| 165/165 [03:22<00:00,  1.23s/it]             \n",
      "[EPOCH 3] Valid Loss : 0.0029, Valid Acc : 0.9651, Valid F1 : 0.8861: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "[EPOCH 4] Train Loss : 0.0278, Train Acc : 0.9022, Train F1 : 0.7715, LR : 9.46e-05: 100%|██████████████████████████| 165/165 [03:23<00:00,  1.23s/it]             \n",
      "[EPOCH 4] Valid Loss : 0.0031, Valid Acc : 0.9575, Valid F1 : 0.8700: 100%|███████████████████████████████████████████| 42/42 [00:24<00:00,  1.71it/s]\n",
      "[EPOCH 5] Train Loss : 0.0275(0.0424), Train Acc : 0.9286(0.8839), Train F1 : 0.8195(0.7310):   2%|▍                  | 4/165 [00:06<04:38,  1.73s/it]             "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3-plant",
   "language": "python",
   "name": "python3-plant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
